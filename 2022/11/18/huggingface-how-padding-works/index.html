<!DOCTYPE html><html lang="ja"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Padding process in HuggingFace transformer models · gojiteji's Blog</title><meta name="description" content="Padding process in HuggingFace transformer models - gojiteji"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://blog.gojtieji.com/atom.xml" title="gojiteji's Blog"><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="gojiteji's Blog" type="application/atom+xml">
</head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://gojiteji.com" target="_blank" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Padding process in HuggingFace transformer models</h1><div class="tags"><a href="/tags/ML/" class="tag-title">#ML</a><a href="/tags/HuggingFace/" class="tag-title">#HuggingFace</a></div><div class="post-info">2022年11月18日</div><div class="post-content"><p> While reading the code for Hugging Face’s T5, I felt uncomfortable with the processing around the paddings, so I examined them below.</p>
<span id="more"></span>
<p>HuggingFace’s attention layer implementation codes are here:<br><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L333">https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L333</a></p>
<h2 id="round-off-error-utilization-How-the-padding-columns-in-attention-matrix-is-converted-to-0"><a href="#round-off-error-utilization-How-the-padding-columns-in-attention-matrix-is-converted-to-0" class="headerlink" title="[round-off error utilization] How the padding columns in attention matrix is converted to 0?"></a>[round-off error utilization] How the padding columns in attention matrix is converted to 0?</h2><p>In the calculation of getting new $V$ in the attention layer, paddings’ attention must be 0 like $A_{i,j_\mathrm{padding}}&#x3D;0$, but I cannot find explicitly substituting 0 in the codes.</p>
<p>This is implemented by utilizing round-off error.<br>As you know, $\mathrm{softmax{(,,-\infty)}&#x3D;(,,0)}.$<br>The variable <code>mask</code> is using minimum value of the type. For example, pytorch’s <code>float</code> is shown below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.finfo(torch.<span class="built_in">float</span>).<span class="built_in">min</span></span><br><span class="line">-<span class="number">3.4028234663852886e+38</span></span><br></pre></td></tr></table></figure>
<p>This is very small number, but it’s not $-\infty$. Then do some errors remind in a calculation result?</p>
<p>The answer is no. The softmax calculation rounds this minimum to zero. Thus, there is no need to compute exactly minus infinity.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>vec=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,torch.finfo(torch.<span class="built_in">float</span>).<span class="built_in">min</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.softmax(vec,<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">0.2689</span>, <span class="number">0.7311</span>, <span class="number">0.0000</span>])</span><br></pre></td></tr></table></figure>
<p>The code is here:</p>
<ul>
<li>type’s minimum value for mask:<br>  <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L529">https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L529</a></li>
<li>softmax round-off error utilization<br>  <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L539">https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L539</a></li>
</ul>
<h2 id="loss-of-trailing-digits-utilization-How-is-the-padding-mask-applied-to-the-original-attention-matrix"><a href="#loss-of-trailing-digits-utilization-How-is-the-padding-mask-applied-to-the-original-attention-matrix" class="headerlink" title="[loss of trailing digits utilization] How is the padding mask applied to the original attention matrix?"></a>[loss of trailing digits utilization] How is the padding mask applied to the original attention matrix?</h2><p> As I mentioned above, the minimum value of the type is assigned in the dimension of the padding in the variable <code>mask</code>. Then, <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L529">the mask is added to position bias</a>, which is <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L538">added to the attention matrix later</a>.</p>
<p> NOTE: <code>mask</code> is a vec here. Adding a vec to the matrix adds a vec to all rows.<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros(<span class="number">3</span>,<span class="number">3</span>)+torch.ones(<span class="number">3</span>)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure></p>
<p>But is it safe to <strong>add</strong> the minimum value (not $-\infty$) instead of assigning it?<br>The answer is yes. This is a utilization of loss of trailing digits.<br>When calculating large and small absolute numbers in the floating point exhibition, python ignores the smaller one. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.finfo(torch.<span class="built_in">float</span>).<span class="built_in">min</span>,  torch.finfo(torch.<span class="built_in">float</span>).<span class="built_in">min</span>+<span class="number">1e3</span></span><br><span class="line">(-<span class="number">3.4028234663852886e+38</span>, -<span class="number">3.4028234663852886e+38</span>)</span><br></pre></td></tr></table></figure>

<p><code>position_bias</code> is so small in absolute value compared to the minimum value of the type that adding them together does not change the fact that they are equivalent to -infinity.</p>
</div></article></div></main><footer><div class="paginator"><a href="/2022/10/29/module/" class="next">NEXT</a></div><div class="copyright"><p>© undefined - 2022 <a href="https://blog.gojtieji.com">gojiteji</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>