<!DOCTYPE html><html lang="ja"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Padding process in HuggingFace transformer models · gojiteji's Blog</title><meta name="description" content="Padding process in HuggingFace transformer models - gojiteji"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://blog.gojtieji.com/atom.xml" title="gojiteji's Blog"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="gojiteji's Blog" type="application/atom+xml">
</head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://gojiteji.com" target="_blank" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Padding process in HuggingFace transformer models</h1><div class="tags"><a href="/tags/ML/" class="tag-title">#ML</a><a href="/tags/HuggingFace/" class="tag-title">#HuggingFace</a></div><div class="post-info">2022年11月18日</div><div class="post-content"><p> While reading the code for Hugging Face’s T5, I felt uncomfortable with the processing around the paddings, so I examined them below.</p>
<span id="more"></span>
<p>HuggingFace’s attention layer implementation codes are here:<br><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L333">https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L333</a></p>
<h2 id="round-off-error-utilization-How-the-padding-columns-in-attention-matrix-is-converted-to-0"><a href="#round-off-error-utilization-How-the-padding-columns-in-attention-matrix-is-converted-to-0" class="headerlink" title="[round-off error utilization] How the padding columns in attention matrix is converted to 0?"></a>[round-off error utilization] How the padding columns in attention matrix is converted to 0?</h2><p>In the calculation of getting new <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D449" xlink:href="#MJX-1-TEX-I-1D449"></use></g></g></g></svg></mjx-container> in the attention layer, paddings’ attention must be 0 like $A_{i,j_\mathrm{padding}}&#x3D;0$, but I cannot find explicitly substituting 0 in the codes.</p>
<p>This is implemented by utilizing round-off error.<br>As you know, $\mathrm{softmax{(,,-\infty)}&#x3D;(,,0)}.$<br>The variable <code>mask</code> is using minimum value of the type. For example, pytorch’s <code>float</code> is shown below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.finfo(torch.<span class="built_in">float</span>).<span class="built_in">min</span></span><br><span class="line">-<span class="number">3.4028234663852886e+38</span></span><br></pre></td></tr></table></figure>
<p>This is very small number, but it’s not $-\infty$. Then do some errors remind in a calculation result?</p>
<p>The answer is no. The softmax calculation rounds this minimum to zero. Thus, there is no need to compute exactly minus infinity.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>vec=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,torch.finfo(torch.<span class="built_in">float</span>).<span class="built_in">min</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.softmax(vec,<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">0.2689</span>, <span class="number">0.7311</span>, <span class="number">0.0000</span>])</span><br></pre></td></tr></table></figure>
<p>The code is here:</p>
<ul>
<li>type’s minimum value for mask:<br>  <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L529">https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L529</a></li>
<li>softmax round-off error utilization<br>  <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L539">https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L539</a></li>
</ul>
<h2 id="loss-of-trailing-digits-utilization-How-is-the-padding-mask-applied-to-the-original-attention-matrix"><a href="#loss-of-trailing-digits-utilization-How-is-the-padding-mask-applied-to-the-original-attention-matrix" class="headerlink" title="[loss of trailing digits utilization] How is the padding mask applied to the original attention matrix?"></a>[loss of trailing digits utilization] How is the padding mask applied to the original attention matrix?</h2><p> As I mentioned above, the minimum value of the type is assigned in the dimension of the padding in the variable <code>mask</code>. Then, <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L529">the mask is added to position bias</a>, which is <a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L538">added to the attention matrix later</a>.</p>
<p> NOTE: <code>mask</code> is a vec here. Adding a vec to the matrix adds a vec to all rows.<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros(<span class="number">3</span>,<span class="number">3</span>)+torch.ones(<span class="number">3</span>)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure></p>
<p>But is it safe to <strong>add</strong> the minimum value (not $-\infty$) instead of assigning it?<br>The answer is yes. This is a utilization of loss of trailing digits.<br>When calculating large and small absolute numbers in the floating point exhibition, python ignores the smaller one. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.finfo(torch.<span class="built_in">float</span>).<span class="built_in">min</span>,  torch.finfo(torch.<span class="built_in">float</span>).<span class="built_in">min</span>+<span class="number">1e3</span></span><br><span class="line">(-<span class="number">3.4028234663852886e+38</span>, -<span class="number">3.4028234663852886e+38</span>)</span><br></pre></td></tr></table></figure>

<p><code>position_bias</code> is so small in absolute value compared to the minimum value of the type that adding them together does not change the fact that they are equivalent to -infinity.</p>
</div></article></div></main><footer><div class="paginator"><a href="/2022/10/29/module/" class="next">下一篇</a></div><div class="copyright"><p>© undefined - 2022 <a href="https://blog.gojtieji.com">gojiteji</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>