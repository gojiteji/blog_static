<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 言語モデルの語彙の違いを見てみる · gojiteji's Blog</title><meta name="description" content="言語モデルの語彙の違いを見てみる - gojiteji"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://blog.gojtieji.com/atom.xml" title="gojiteji's Blog"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="gojiteji's Blog" type="application/atom+xml">
</head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://gojiteji.com" target="_blank" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">言語モデルの語彙の違いを見てみる</h1><div class="tags"><a href="/tags/NLP/" class="tag-title">#NLP</a><a href="/tags/AI/" class="tag-title">#AI</a></div><div class="post-info">Dec 14, 2022</div><div class="post-content"><blockquote>
<p><strong>これは<a target="_blank" rel="noopener" href="https://qiita.com/advent-calendar/2022/naist">NAISTアドベントカレンダー2022</a> 1日目の代理投稿です。</strong></p>
</blockquote>
<p>「語彙力がない」「ボキャ貧」なんて言葉を人間は使いますが，AIにもそういった特徴はあるのでしょうか？少し気になったので検証してみました．やり方は簡単で，言語モデルごとの語彙の集合を用いてベン図を描きました．</p>
<p>ソースコード：<a target="_blank" rel="noopener" href="https://github.com/gojiteji/AI_vocab_comparison/blob/main/Vocab_comparison.ipynb">https://github.com/gojiteji/AI_vocab_comparison/blob/main/Vocab_comparison.ipynb</a></p>
<p>注意：<br>トークン化手法ごとにprefixが違う場合あります．異なるトークン化手法を用いているものは，prefixは全て削除しました．従って，後続トークンとしてのback(ex. feedback)と，先頭トークンとbackが同一トークンとして扱われます．また，extra idや言語コード，special tokenは削除しておりません．</p>
<span id="more"></span>
<h2 id="T5-vs-mT5"><a href="#T5-vs-mT5" class="headerlink" title="T5 vs mT5"></a>T5 vs mT5</h2><table>
<thead>
<tr>
<th>モデル名</th>
<th>トークン化手法</th>
<th>データセット</th>
<th>リンク</th>
</tr>
</thead>
<tbody><tr>
<td>T5</td>
<td>SentencePiece</td>
<td>C4</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/t5-small">https://huggingface.co/t5-small</a></td>
</tr>
<tr>
<td>mT5</td>
<td>SentencePiece</td>
<td>mC4</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/google/mt5-small">https://huggingface.co/google/mt5-small</a></td>
</tr>
</tbody></table>
<p><img src="https://blog.gojiteji.com/images/vocab/T5vsmT5.png" width="50%"></img></p>
<p>データセットがC4より多いmC4を使っているだけあって，mT5の方が語彙サイズがとても大きいことがわかります．</p>
<h2 id="mBART25-vs-mBART50-vs-mT5"><a href="#mBART25-vs-mBART50-vs-mT5" class="headerlink" title="mBART25 vs mBART50 vs mT5"></a>mBART25 vs mBART50 vs mT5</h2><table>
<thead>
<tr>
<th>モデル名</th>
<th>トークン化手法</th>
<th>データセット</th>
<th>リンク</th>
</tr>
</thead>
<tbody><tr>
<td>mBART25</td>
<td>SentencePiece</td>
<td>Common Crawl</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/facebook/facebook/mbart-large-cc25">https://huggingface.co/facebook/facebook/mbart-large-cc25</a></td>
</tr>
<tr>
<td>mBART50</td>
<td>SentencePiece</td>
<td>Same as mBART25</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/google/facebook/mbart-large-50">https://huggingface.co/google/facebook/mbart-large-50</a></td>
</tr>
<tr>
<td>mT5</td>
<td>SentencePiece</td>
<td>mC4</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/google/mt5-small">https://huggingface.co/google/mt5-small</a></td>
</tr>
</tbody></table>
<p><img src="https://blog.gojiteji.com/images/vocab/mBART25vsmBART50vsmT5.png" width="50%"></img></p>
<p>データセットはいずれも CommonCrawl で，mBART25とmBART50はトークナイザは言語コードの分が違うだけのようですね<br>ちなみに，異なる27個の語彙は以下の通りです．</p>
<blockquote>
<p>‘af_ZA’,’az_AZ’,’bn_IN’,’fa_IR’,’gl_ES’,’he_IL’,’hr_HR’,’id_ID’,’ka_GE’,’km_KH’,’mk_MK’,’ml_IN’,’mn_MN’,’mr_IN’,’pl_PL’,’ps_AF’,’pt_XX’,’sl_SI’,’sv_SE’,’sw_KE’,’ta_IN’,’te_IN’,’th_TH’,’tl_XX’,’uk_UA’,’ur_PK’,’xh_ZA’</p>
</blockquote>
<h2 id="GPT-J-vs-GPT-2-x2F-3"><a href="#GPT-J-vs-GPT-2-x2F-3" class="headerlink" title="GPT-J vs GPT-2&#x2F;3"></a>GPT-J vs GPT-2&#x2F;3</h2><p>これは<a target="_blank" rel="noopener" href="https://github.com/kingoflolz/mesh-transformer-jax/">GPT-JのGitHubに書いてありますが</a>，GPT-2&#x2F;3と同じtokenizer(Byte Pair Encoding)を使っているそうです．(以下略)</p>
<h2 id="GPT2-japanese-vs-BERT-japanese-vs-T5-japanese"><a href="#GPT2-japanese-vs-BERT-japanese-vs-T5-japanese" class="headerlink" title="GPT2-japanese vs BERT-japanese vs T5-japanese"></a>GPT2-japanese vs BERT-japanese vs T5-japanese</h2><table>
<thead>
<tr>
<th>モデル名</th>
<th>トークン化手法</th>
<th>データセット</th>
<th>リンク</th>
</tr>
</thead>
<tbody><tr>
<td>GPT2-japanese</td>
<td>SentencePiece</td>
<td>Wikipedia(ja)</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/rinna/japanese-gpt2-medium">https://huggingface.co/rinna/japanese-gpt2-medium</a></td>
</tr>
<tr>
<td>BERT-japanese</td>
<td>WordPiece</td>
<td>IPA dictionary</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/cl-tohoku/bert-base-japanese">https://huggingface.co/cl-tohoku/bert-base-japanese</a></td>
</tr>
<tr>
<td>T5-japanese</td>
<td>SentencePiece</td>
<td>Wikipedia(ja),OSCAR(ja),CC-100(ja)</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/sonoisa/t5-base-japanese">https://huggingface.co/sonoisa/t5-base-japanese</a></td>
</tr>
</tbody></table>
<p><img src="https://blog.gojiteji.com/images/vocab/GPTjavsBERTjavsT5ja.png" width="50%"></img></p>
<p>同じ日本語wikipediaをトークナイザの学習に使っているため，GPT2-japaneseとT5-japaneseの共通部分が大きいですね．微妙に異なる部分がるのは，special tokensやdumpの時期の違いによるものでしょうか．（何かご存知の方いらっしゃったらご教授ください）</p>
<h2 id="最後に"><a href="#最後に" class="headerlink" title="最後に"></a>最後に</h2><p>モデルとトークナイザは学習が独立していることが多いため，トークナイザが単体でどのような挙動をしているかを知っておくことも重要なのだと思いました．この辺もちゃんと理解して「ニューラルネットの気持ち」になりたいですね．</p>
</div></article></div></main><footer><div class="paginator"><a href="/2022/12/21/nagi-system/" class="prev">PREV</a><a href="/2022/12/02/adventcalendar2022/" class="next">NEXT</a></div><div class="copyright"><p>© 2020 - 2023 <a href="https://blog.gojtieji.com">gojiteji</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>