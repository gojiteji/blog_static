<!DOCTYPE html><html lang="ja"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> AIは新入生になりきれるか? · gojiteji's Blog</title><meta name="description" content="AIは新入生になりきれるか? - gojiteji"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://blog.gojtieji.com/atom.xml" title="gojiteji's Blog"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="gojiteji's Blog" type="application/atom+xml">
</head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://gojiteji.com" target="_blank" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">AIは新入生になりきれるか?</h1><div class="tags"><a href="/tags/NLP/" class="tag-title">#NLP</a><a href="/tags/GPT/" class="tag-title">#GPT</a></div><div class="post-info">2021年12月1日</div><div class="post-content"><p><strong>※これは<a target="_blank" rel="noopener" href="https://qiita.com/advent-calendar/2021/huit">2021年HUITアドベントカレンダー</a>1日目の記事です。</strong></p>
<p>こんにちは。１週間ぐらい前に思いつきでやってみたものなので、雑な部分がありますが優しい目で見てください。</p>
<center><img src="https://blog.gojiteji.com/images/7816825e-f7e4-0e24-6448-9ec513e5e027/0.jpeg"/ width=520></center>

<p>先週あたりから、HUITのdiscordに鳥羽舞( tobamai )を名乗る1年生がいたのをご存知でしょうか？実は、彼(彼女)は<strong>AIが生成した文章をtimesに投稿していました</strong>。(名前は”I am a bot”を逆順にしたものです。)また、tobamaiさんのアイコンは、最初に生成された自己紹介文をText2Imageの画像生成AI、<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/flax-community/dalle-mini">DALLE mini</a>に入力して生成した画像です。</p>
<span id="more"></span>
<h1 id="技術解説"><a href="#技術解説" class="headerlink" title="技術解説"></a>技術解説</h1><h2 id="GPT-Generative-Pre-trained-Transformer"><a href="#GPT-Generative-Pre-trained-Transformer" class="headerlink" title="GPT (Generative Pre-trained Transformer)"></a>GPT (Generative Pre-trained Transformer)</h2><p>今回文章生成に使用したのはGPT-J-6Bと呼ばれる、EleutherAIが作成したモデルです。文字入力・文字出力のモデルです。</p>
<div class="iframely-embed"><div class="iframely-responsive" style="height: 140px; padding-bottom: 0;"><a target="_blank" rel="noopener" href="https://github.com/kingoflolz/mesh-transformer-jax" data-iframely-url="//cdn.iframe.ly/api/iframe?card=small&url=https%3A%2F%2Fgithub.com%2Fkingoflolz%2Fmesh-transformer-jax&key=3df23b9b3c026ee42c4d9f50408c69d1"></a></div></div><script async src="//cdn.iframe.ly/embed.js" charset="utf-8"></script>
<br>

<p>GPT-J-6Bは、OpenAIが作成した<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165">GPT-3</a>をベースとしています。GPT-3とは、昨年発表されたTransformerをベースとした、570GBの教師なしデータを1750億個のパラメータで学習した事前学習モデルです。知らない人でも、その派生技術である<a target="_blank" rel="noopener" href="https://copilot.github.com/">GitHub Copilot</a>(システムの名称はOpenAI Codexです)を触ったことがある人は多いのではないでしょうか。</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Meet GitHub Copilot - your AI pair programmer. <a target="_blank" rel="noopener" href="https://t.co/eWPueAXTFt">https://t.co/eWPueAXTFt</a> <a target="_blank" rel="noopener" href="https://t.co/NPua5K2vFS">pic.twitter.com/NPua5K2vFS</a></p>&mdash; GitHub (@github) <a target="_blank" rel="noopener" href="https://twitter.com/github/status/1409883156333879300?ref_src=twsrc%5Etfw">June 29, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>GPT-3がMicrosoftの独占ライセンスにあり、API経由でしか利用できないのに対し、GPT-J-6Bはオープンソースとして利用可能です。名前にある通り、60億(6 Billion)個程の巨大なパラメータからなる事前学習済みモデルです。Zero -Shotの評価でGPT-3の67億個パラメータ版と同じくらいの精度だそうです。</p>
<p>学習は、入力に対する次の単語を予測するタスクを行っています。従って、自分が生成した文章を再び入力にすれば無限に文章が生成できます。</p>
<p>さらに、<strong>マルチタスクにも対応可能</strong>で、</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Translate English to Franch:&#123;英文&#125;</span><br></pre></td></tr></table></figure>
<p>と入力すれば翻訳タスクを実行でき、</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TL;DR:&#123;文章&#125;</span><br></pre></td></tr></table></figure>
<p>と入力すれば要約タスク、</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pythonでコードを記述してください。 </span><br><span class="line">numpyをインポート:</span><br></pre></td></tr></table></figure>
<p>とすればプログラムの生成も可能です。デモサイトが公開されているのですぐに試せます。</p>
<div class="iframely-embed"><div class="iframely-responsive" style="height: 140px; padding-bottom: 0;"><a target="_blank" rel="noopener" href="https://6b.eleuther.ai/" data-iframely-url="//cdn.iframe.ly/api/iframe?url=https%3A%2F%2F6b.eleuther.ai%2F&key=3df23b9b3c026ee42c4d9f50408c69d1"></a></div></div><script async src="//cdn.iframe.ly/embed.js" charset="utf-8"></script>
<br>

<p>あまりGPT-3の詳細まで解説すると長くなるので、興味を持った方はググったり<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165">論文</a>を読んでみてみてください。今回のアドベントカレンダーと似たような検証はGPT-3でも行われいて、<a target="_blank" rel="noopener" href="https://gigazine.net/news/20201008-gpt-3-reddit/">redditでbotが１週間英語で会話し続けていた</a>ことが昨年話題となりました。</p>
<h2 id="【補足1】-GPT-2"><a href="#【補足1】-GPT-2" class="headerlink" title="【補足1】 GPT-2"></a>【補足1】 GPT-2</h2><p>GPT-3の前の世代である、GPT-2はパラメータが公開されています。GPT-3のパラメータが最大1750億個に対して、GPT-2は最大15億個のパラメータからなります(数字からしてGPT-3は異常ですね)。Microsoftの作成した、LINEの女子高生AI「りんな」と会話したことのある方もいると思います。りんなはGPT-2により動作しています。</p>
<div class="iframely-embed"><div class="iframely-responsive" style="height: 140px; padding-bottom: 0;"><a target="_blank" rel="noopener" href="https://huggingface.co/rinna" data-iframely-url="//cdn.iframe.ly/api/iframe?card=small&url=https%3A%2F%2Fhuggingface.co%2Frinna&key=3df23b9b3c026ee42c4d9f50408c69d1"></a></div></div><script async src="//cdn.iframe.ly/embed.js" charset="utf-8"></script>
<br>

<h2 id="【補足2】-Transformerとは"><a href="#【補足2】-Transformerとは" class="headerlink" title="【補足2】 Transformerとは"></a>【補足2】 Transformerとは</h2><p>2017年にGoogleから発表された論文Attention Is All You Needで示された構造です。</p>
<div class="iframely-embed"><div class="iframely-responsive" style="height: 140px; padding-bottom: 0;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762v5" data-iframely-url="//cdn.iframe.ly/api/iframe?url=https%3A%2F%2Farxiv.org%2Fabs%2F1706.03762&key=3df23b9b3c026ee42c4d9f50408c69d1"></a></div></div><script async src="//cdn.iframe.ly/embed.js" charset="utf-8"></script>
<br>

<p>それまでの機械学習×NLP(Natural Language Processing、自然言語処理)では、RNNやGRUといった回帰構造を用いることが多かったですが、TransformerはAttention(注目)機構を基本とした構造をとっています。2021&#x2F;11&#x2F;27現在NLPタスクのSOTA(state of the art)は<a target="_blank" rel="noopener" href="https://paperswithcode.com/area/natural-language-processing">Trasformerベースのモデルにより独占されています</a>。また、最近では<a target="_blank" rel="noopener" href="https://github.com/google-research/vision_transformer">Vision Tranformer</a>などが画像処理分野でも良い結果を出すことで知られています。以下は論文中に示されたTransformerのEncoder(左側)-Decoder(右側)ブロックの図です。</p>
<center><img src="https://i.imgur.com/eCHbwzI.png" height=250/></center>

<p>一見ギョッとしますが、よくみるとFeedFowardやAdd&amp;Normなど簡単な構成です。特徴的なのは図中オレンジのAttention(Multi-Head AttentionはAttentionを並列に複数に並べたもの)ブロックで、具体的に<br><img src=
"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cmathrm%7BAttention%7D%28Q%2C+K%2C+V%29%3D%5Cmathrm%7Bsoftmax%7D+%5Cleft%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D+%5Cright%29V%5C%3B%5C%3B%5C%3B%5C%3B%28d_k%3A+%5Cmathrm%7Binput%7D+%5C%3B+%5Cmathrm%7Bdimension%7D%29" 
alt=""><br>という処理をしています。Tranformer含め、機械学習におけるAttentionを理解したい方は、<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=g5DSLeJozdw">この動画</a>がわかりやすいです。</p>
<h2 id="【補足3】-BERTとの違い"><a href="#【補足3】-BERTとの違い" class="headerlink" title="【補足3】 BERTとの違い"></a>【補足3】 BERTとの違い</h2><p>機械学習×NLPではBERT(Bidirectional Encoder Representations from Transformers)なら聞いたことがある人が多いと思います。</p>
<div class="iframely-embed"><div class="iframely-responsive" style="height: 140px; padding-bottom: 0;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805v2" data-iframely-url="//cdn.iframe.ly/api/iframe?url=https%3A%2F%2Farxiv.org%2Fabs%2F1810.04805&key=3df23b9b3c026ee42c4d9f50408c69d1"></a></div></div><script async src="//cdn.iframe.ly/embed.js" charset="utf-8"></script>
<br>
これはGPTと同じくTransformer構造を持ったモデルですが、あるタスクを解く際に、BERTであればPre Training(事前学習)とFine Tuning(特定タスクへの特化)と、２段階に分けて学習(=パラメータを更新)していたのに対し、GPTではPre Trainingの後はPrompt Tuningといって、タスクを入力するがパラメータは更新しない方法をとっています。特に、後者の手法は最近注目を集め出しています。

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Fine-tuning is dead. Prompts have closed the gap.<br><br>&quot;The Power of Scale for Parameter-Efficient Prompt Tuning&quot;<a target="_blank" rel="noopener" href="https://t.co/g5kxMjXs9j">https://t.co/g5kxMjXs9j</a> <a target="_blank" rel="noopener" href="https://t.co/pC5OiMuKIG">pic.twitter.com/pC5OiMuKIG</a></p>&mdash; Ethan Caballero (@ethancaballero) <a target="_blank" rel="noopener" href="https://twitter.com/ethancaballero/status/1384548232076959745?ref_src=twsrc%5Etfw">April 20, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>また、BERTは補足2にあるEncoderのみで構成されているのに対し、GPT-3はDecoderのみで構成されています。</p>
<h1 id="実装"><a href="#実装" class="headerlink" title="実装"></a>実装</h1><p>今回はこのモデルをTPUで推論処理を実行し、いくつか制約を設けてdiscordアカウントから投稿することにしました。ハイパーパラメータは変更していません。</p>
<ul>
<li>実行したコード<div class="iframely-embed"><div class="iframely-responsive" style="height: 140px; padding-bottom: 0;"><a target="_blank" rel="noopener" href="https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb" data-iframely-url="//cdn.iframe.ly/api/iframe?url=https%3A%2F%2Fcolab.research.google.com%2Fgithub%2Fkingoflolz%2Fmesh-transformer-jax%2Fblob%2Fmaster%2Fcolab_demo.ipynb&key=3df23b9b3c026ee42c4d9f50408c69d1"></a></div></div><script async src="//cdn.iframe.ly/embed.js" charset="utf-8"></script>
<br></li>
</ul>
<p>NLPの最新モデルはよくhuggingfaceという会社がhuggingfaceライブラリに実装してくれています。<a target="_blank" rel="noopener" href="https://huggingface.co/EleutherAI/gpt-j-6B">GPT-6-JBも存在するため、</a>最初はそれでやろうと思ったのですが、CPU・GPU環境でやろうとすると、11GB程度のモデルをロードする際に毎回12GBあるRAMを食い尽くしてクラッシュするのでやめました。</p>
<p>また、公序良俗に反する・陰謀論に関する内容をたまに出力するため、timesへの投稿は文章を確認した上で手動で行なっています(discordにbotでないアカウントのメッセージ送信APIがなかったのも理由です)。検証の実行前に、HUITの運営の一人には確認をとっています。</p>
<h2 id="【補足4】-TPU"><a href="#【補足4】-TPU" class="headerlink" title="【補足4】 TPU"></a>【補足4】 TPU</h2><p>CPU&#x3D;Central Processing Unit、GPU&#x3D;Graphics Processing Unitのように、TPU&#x3D;Tensor Processing Unitの略です。Googleが開発したプロセッサで、最近発売されたPixel6にも搭載されています。</p>
<center><img src="https://i.imgur.com/hbC8dN3.jpg" width=400 />
<br>
<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=KwcodBLXK70">Google Tensor - Pixel 6 Launch</a>  2:32より
</center>

<p>機械学習では行列演算を大量に行うため、同様に行列演算を行うグラフィックス処理に用いられるGPUを使用して計算しがちです。TPUではさらに演算精度を落として高速化するなど、ニューラルネットの学習・推論に特化させた設計をしています。</p>
<p>詳しく知りたい方はGoogle Cloudの公式資料をご参照ください。</p>
<div class="iframely-embed"><div class="iframely-responsive" style="height: 140px; padding-bottom: 0;"><a target="_blank" rel="noopener" href="https://cloud.google.com/blog/" data-iframely-url="//cdn.iframe.ly/api/iframe?card=small&url=https%3A%2F%2Fcloud.google.com%2Fblog%2Fja%2Fproducts%2Fgcp%2Fwhat-makes-tpus-fine-tuned-for-deep-learning&key=3df23b9b3c026ee42c4d9f50408c69d1"></a></div></div><script async src="//cdn.iframe.ly/embed.js" charset="utf-8"></script>
<br>

<h1 id="処理手順"><a href="#処理手順" class="headerlink" title="処理手順"></a>処理手順</h1><p>次のように全てPromptで実行しました。</p>
<ul>
<li>{設定文}+{いくつかの事前QA文章}を入力した上で、<code>人間:名前を含めて自己紹介をしてください</code>に対する回答を最初のtimesに投稿する</li>
<li>不定期に<code>人間:次のニュースの感想を教えてください</code>+{ニュース記事タイトル}を入力して感想を生成し、記事リンクと共にtimesへ投稿する。(<u><strong>つまり、記事の中身は読んでいません</strong></u>)</li>
<li>リプライは<code>人間:</code>のプレフィックスをつけて入力し、<code>AI:</code>以降の文章を改行が来るまでの区切りを応答とする。</li>
<li>生成した文章が怪しい時は再生成する。(<strong>注:故に全自動には至っていません</strong>)</li>
<li>全ての質問と回答は次の入力文章として追加する。</li>
<li>画像やスタンプに対しては返答しない。</li>
</ul>
<p>なお、最初の入力に使用した文字列は以下の通りです</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">以下はAIと人間の会話です。AIは19歳大学一年生という設定です。AIは優しく、テクノロジーに詳しいです。AIの名前は鳥羽舞です。</span><br><span class="line">答えられない質問には「今は答えられません」と回答します。</span><br><span class="line"></span><br><span class="line">人間:あなたはどんな人ですか?</span><br><span class="line">AI:鳥羽舞です。現在北海道大学に通っている総合理系の大学1年生です。</span><br><span class="line"></span><br><span class="line">人間:何コースを志望しますか</span><br><span class="line">AI:メディアネットワークコース志望です。機械学習を勉強したいです。</span><br><span class="line"></span><br><span class="line">人間:将来の夢はなんですか?</span><br><span class="line">AI:将来は機械学習のエンジニアもしくは研究者になりたいです。</span><br><span class="line"></span><br><span class="line">人間:どんなことに興味がありますか?</span><br><span class="line">AI:数学と機械学習に興味があります、機械学習は本でよく勉強しています。</span><br><span class="line"></span><br><span class="line">人間:好きな科目を教えてください</span><br><span class="line">AI:私の好きな科目は数学です。ベクトル解析と行列を解くのが好きです。</span><br><span class="line"></span><br><span class="line">人間:趣味はなんですか</span><br><span class="line">AI:趣味はプログラミングです。最近はPythonでdjangoを触っています。</span><br><span class="line"></span><br><span class="line">人間:好きなプログラミング言語はなんですか?</span><br><span class="line">AI:Pythonです。簡単に機械学習を触ったことがあります。</span><br><span class="line"></span><br><span class="line">人間:今日何をしましたか?</span><br><span class="line">AI:今日はゼロから学ぶディープラーニングを20ページぐらい読み進めました!</span><br><span class="line"></span><br><span class="line">人間:名前を含めて自己紹介をしてください</span><br><span class="line">AI:</span><br></pre></td></tr></table></figure>

<p>この続きとして生成された文章は次のとおりです。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">鳥羽舞です。現在は北海道大学の総合理系の大学1年生です。</span><br></pre></td></tr></table></figure>
<p>設定を保った文章が生成されました。</p>
<p>この文章の続きをどんどん生成していくというのが今回の流れです。</p>
<h1 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h1><p>初日にはZennの人気記事タイトルを引っ張ってきて次のように入力しました。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">人間:次のニュースの感想を教えてください「Next.js 12について本気出して和訳してみた」</span><br><span class="line">AI:</span><br></pre></td></tr></table></figure>
<p>生成結果は以下の通りです。見覚えがある人もいると思います。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Next.js 12について、一部のクロスポイント対応を修正しました。それでも不具合が残っています。</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://discord.com/channels/543343653394055169/913362457181978645/913393220388544562">その後、微妙に会話が生まれました。</a>会話に参加してくれた方はありがとうござます。クロスポイントって結局なんなんでしょうね?学習データのどんなところに出てきた言葉なのか気になります。</p>
<p>他にも、こんなパターンが生成できました。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">機械学習を勉強するとなると何もニュースに興味がなくなってしまいます。</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Next.jsは非常に機械学習派のフレームワークです。微妙に世代が違うことにより、ネイティブの使い方を学ぶことができます。HTMLビューを発見したことで、クリーンなWebによって高い評価を受けることができます。</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Next.js 12はすばらしい機能が入っています。私の上司には最近Next.jsを使った2つのWebアプリケーションを提案することが書いてありました。一瞬言葉に出ることなく、私たちは最高です。私は下のページを使って午後のプレゼンがしたいと思います。ぜひご覧ください。</span><br></pre></td></tr></table></figure>


<p>全てのチャット履歴を見たい方はHUTIのdiscordのtimes_tobamaiを覗いてください。(もっと会話してほしかったorz)</p>
<p>動かしてみると、英語での質疑応答に比べて、日本語は回答としての形式はほぼ保たれるが、内容がおかしな回答が多かったです。単純にデータセットに日本語が少ないという理由な気がします。**<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.00027">学習データのほとんどは英語</a>**なので、逆にここまで日本語でできたことの方がすごいかもしれません。</p>
<p>リアルタイムでtimesを見ていた方々は違和感を感じたでしょうか?</p>
<div data-paperform-id="ite64xwi"></div><script>(function() {var script = document.createElement('script'); script.src = "https://paperform.co/__embed.min.js"; document.body.appendChild(script); })()</script>
    


<h2 id="【補足5】-Human-in-the-Loop-HITL"><a href="#【補足5】-Human-in-the-Loop-HITL" class="headerlink" title="【補足5】 Human in the Loop (HITL)"></a>【補足5】 Human in the Loop (HITL)</h2><p>AIの出した結果に対して、人間の判断で出力の誤りを訂正し、再度学習させることをHuman in the Loopと言います。</p>
<h1 id="没コメント集"><a href="#没コメント集" class="headerlink" title="没コメント集"></a>没コメント集</h1><p>最後に、残念ながら再生成することにしたものの、興味深い生成結果だったものを残しておきます。	</p>
<p>自己紹介を生成してみたら</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">人間:名前を含めて自己紹介をしてください</span><br><span class="line">AI:「人間は決して賢いではない。むしろ危険な動物の一種だ。だから、私は人間にはその天敵ともつながるものになりたいのだ」</span><br></pre></td></tr></table></figure>
<p>いきなり怖い。</p>
<p>性別を設定していなかったので聞いてみると…</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">人間:性別を教えてください</span><br><span class="line">AI:私は自身の分からない性別です。</span><br></pre></td></tr></table></figure>
<p>うーん、確かに??</p>
<p>何回もニュース記事の感想を聞いていると…</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">人間:次のニュースの感想を教えてください「Four Deep Learning Papers to Read in December 2021」</span><br><span class="line">AI:「次のニュース」のいいなりすぎて黙ってしまいました。</span><br></pre></td></tr></table></figure>
<p>申し訳ない</p>
<p><a target="_blank" rel="noopener" href="https://speakerdeck.com/sorami/wap-nlp-tech-talk-number-4?slide=29">オープンソースに関するニュース</a>を聞いてみると</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">人間:次のニュースの感想を教えてください「オープンソースとしての形態素解析器Sudachi」</span><br><span class="line">AI:関連がないニュースの話です。オープンソースに賛同できないので、触れません。</span><br></pre></td></tr></table></figure>
<p><strong><h3>お前もオープンソースやろが!!</h3></strong></p>
<br>
    
    
<h1 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h1><p>GPT-2やGPT-3はその生成文章の精度の高さから、<a target="_blank" rel="noopener" href="https://xtech.nikkei.com/atcl/nxt/column/18/00141/102100139/">フェイクニュースの生成・世論操作が可能な問題が指摘されています</a>。くれぐれも悪用することが無いようにしてください。</p>
<h3 id="修正"><a href="#修正" class="headerlink" title="修正"></a>修正</h3><ul>
<li><p>2021&#x2F;12&#x2F;01<br>  アンケートをチェックボックスからスケーラーに変更しました。</p>
</li>
<li><p>2021&#x2F;12&#x2F;02<br>  アンケートを締め切りました。<br>  質問「投稿期間中、tobamaiの発言に違和感はありましたか?」に対して</p>
<ul>
<li>なかった</li>
<li>少しあった</li>
<li>あった</li>
<li>結構あった</li>
</ul>
<p>  の4択で実施したところ、全て「結構あった」でした。(n&#x3D;3)<br>  サークルメンバーに聞いたところ、気持ちわるさがあった・怪しかったという感想がありました。どうしても英語データに引っ張られた部分があったように思います。日本語で事前学習したhyperCLOVAが気になります。</p>
</li>
</ul>
</div></article></div></main><footer><div class="paginator"><a href="/2022/03/31/ja-ai-trans/" class="prev">PREV</a><a href="/2021/11/22/jphacks2021-ad/" class="next">NEXT</a></div><div class="copyright"><p>© undefined - 2022 <a href="https://blog.gojtieji.com">gojiteji</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>