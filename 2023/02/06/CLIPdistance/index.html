<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> CLIPにおける1トークンとその文字列の距離 · gojiteji's Blog</title><meta name="robots" content="noindex"><meta name="description" content="CLIPにおける1トークンとその文字列の距離 - gojiteji"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://blog.gojtieji.com/atom.xml" title="gojiteji's Blog"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="gojiteji's Blog" type="application/atom+xml">
</head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://gojiteji.com" target="_blank" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">CLIPにおける1トークンとその文字列の距離</h1><div class="tags"><a href="/tags/NLP/" class="tag-title">#NLP</a></div><div class="post-info">Feb 6, 2023</div><div class="post-content"><h2 id="イントロ"><a href="#イントロ" class="headerlink" title="イントロ"></a>イントロ</h2><center><img src="https://blog.gojiteji.com/images/clip/applesign.png"/ width=520></center>

<p>Stable diffusion[1]では，<code>A road sign with the word &quot;apple&quot;</code> と入力すれば，上の画像のように，文字を画像化することが可能です．<br>一方で，「意味に対応する文字っぽいもの」がそれとなく表示されるものの，果たして本当に”文字”として学習しているのか？という疑問を持ったため，簡単に実験してみました．</p>
<p>Character-Aware Models Improve Visual Text Rendering.[3]では，画像生成とその画像文字の質に関する精度が大規模に実験されています．中でも，Stable Diffusionや，Imagen[4]では，文字画像生成にはにはミスが多く存在し，画像生成にバイトレベルTransformer(ByT5[5])を用いると，文字の生成クオリティが向上したとの結果が報告されています．</p>
<p>今回は，文字列とトークンがどれだけ対応しているかを，トークンの文字列情報と，トークンの埋め込み情報を用いて検証してみます．</p>
<span id="more"></span>


<h2 id="実験"><a href="#実験" class="headerlink" title="実験"></a>実験</h2><p>今回の目標はトークンの文字情報の編集距離とトークンの隠れベクトルのユークリッド距離が，どの程度異なるのか，を調べることです．<br>今回使うモデルは，Stable Diffusionに使われているCLIPのテキストエンコーダです．CLIPの語彙に対して．全ての語彙から2組の組み合わせに対して距離を計算してどんな関係になっているのかをみてみます．<br>なるべく埋め込みに文字情報を持たせたいので，promptは，<code>A road sign with the word &quot;&#123;単語&#125;&quot;</code>と入力し，単語部分のみを埋め込みを利用します．</p>
<p>以下，主に計算コストからデータを減らします．</p>
<ul>
<li>文字列長が文字列種類の80%以上が異なる文字からなる単語同士の比較は，本題の趣旨から逸れるため計算処理から外しています．</li>
<li>計算コストや生起率の観点から10文字以上のトークンは計算に含めていません．</li>
<li>複雑すぎる文字の編集も，本誌の趣旨がら逸れるため，編集距離は4以下のものを計算しています．．<br>実験コードはGitHubにあげています．修正点等ありましたら，PRやissueで連絡ください．<br><a target="_blank" rel="noopener" href="https://github.com/gojiteji/CLIPdistance">https://github.com/gojiteji/CLIPdistance</a></li>
</ul>
<h2 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h2><p>グラフに書くと以下のようになりました．</p>
<p>編集距離と言っても，削除や挿入と，置換は埋め込みベクトルに対してかなり性質が異なる気がするので，それら個別の操作に対する埋め込みの変化も気になるところです．</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2021. High-Resolution Image Synthesis with Latent Diffusion Models.<br>[2] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. arXiv.<br>[3] Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, R. J. Mical, Mohammad Norouzi, and Noah Constant. 2022. Character-Aware Models Improve Visual Text Rendering. arXiv.<br>[4] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.<br>[5] Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2022. ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models. Transactions of the Association for Computational Linguistics, 10:291–306.</p>
<!-- flag of hidden posts --></div></article></div></main><footer><div class="paginator"></div><div class="copyright"><p>© 2020 - 2023 <a href="https://blog.gojtieji.com">gojiteji</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>