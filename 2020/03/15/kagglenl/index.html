<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Real or Not? NLP with Disaster Tweets を解く · gojiteji's Blog</title><meta name="description" content="Real or Not? NLP with Disaster Tweets を解く - gojiteji"><title>gojiteji's Blog</title><meta name="description" content="gojiteji's Blog"><meta name="twitter:title" content="gojiteji's Blog"><meta name="twitter:description" content="gojiteji's Blog"><meta name="og:title" content="gojiteji's Blog"><meta name="og:description" content="gojiteji's Blog"><meta name="og:image"><meta name="twitter:image"><meta name="twitter:site" content="@gojiteji"><meta name="twitter:creator" content="@gojiteji"><meta name="og:site_name" content="gojiteji's Blog"><meta name="og:type" content="article"><meta name="og:url" content="https://blog.gojtieji.com/2020/03/15/kagglenl/"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://blog.gojtieji.com/atom.xml" title="gojiteji's Blog"><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="gojiteji's Blog" type="application/atom+xml">
</head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://gojiteji.com" target="_blank" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Real or Not? NLP with Disaster Tweets を解く</h1><div class="post-info">Mar 15, 2020</div><div class="post-content"><p>前回、NNで翻訳を作ってみたのですが、モデルの評価等をすっ飛ばしてきました。そこで、今回はkaggleで現在開いているNLPの問題を、検証含め解いてみたいと思います。</p>
<h2 id="問題内容"><a href="#問題内容" class="headerlink" title="問題内容"></a>問題内容</h2><p>解いた問題はReal or Not? NLP with Disaster Tweets(<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/nlp-getting-started">https://www.kaggle.com/c/nlp-getting-started</a> ) です。ツイート内容からを本物の災害かどうかを2値推定するというものです。精度を上げれば、どんな災害がいつどこで起きているかをツイッターの呟きから特定する、ソーシャルセンサーとして活用できる気もします。<br>先に結果を書いておくと、スコアは0.74539でした。</p>
<span id="more"></span>
<h2 id="実装"><a href="#実装" class="headerlink" title="実装"></a>実装</h2><p>モデルの概要図は下の通りです。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">  text</span><br><span class="line">   ↓</span><br><span class="line">embedding</span><br><span class="line">   ↓</span><br><span class="line">Positional Embedding</span><br><span class="line">   ↓</span><br><span class="line">Encoder Layers</span><br><span class="line">   ↓</span><br><span class="line"> linear ← embedding ← keyword</span><br><span class="line">   ↓</span><br><span class="line">Sigmoid</span><br><span class="line">   ↓</span><br><span class="line">  出力</span><br></pre></td></tr></table></figure>
<p>前回同様,attentionベースで作成しました。ただ、翻訳ではないので、使用した注意機構はself-attentionのみとなります。</p>
<h3 id="全体概要"><a href="#全体概要" class="headerlink" title="全体概要"></a>全体概要</h3><p>コードは ( <a target="_blank" rel="noopener" href="https://github.com/gojiteji/kaggleNL">https://github.com/gojiteji/kaggleNL</a> ) にあげてるので、必要な部分を順を追って解説していきます。</p>
<h3 id="データ作成部分"><a href="#データ作成部分" class="headerlink" title="データ作成部分"></a>データ作成部分</h3><p>まずは辞書クラスを作ります。decoder機能(id-&gt;word変換)は使用しませんが、デバッグなど確認用に、前回同様の実装を行います。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Vocabulary</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,disabletokenize=<span class="literal">False</span></span>):</span><br><span class="line">        self.w2i=&#123;&#125;</span><br><span class="line">        self.i2w=&#123;&#125;</span><br><span class="line">        self.oov_char=<span class="string">&#x27;&lt;unk&gt;&#x27;</span></span><br><span class="line">        self.pad=<span class="string">&#x27;&lt;pad&gt;&#x27;</span></span><br><span class="line">        self.bos=<span class="string">&#x27;&lt;bos&gt;&#x27;</span></span><br><span class="line">        self.eos=<span class="string">&#x27;&lt;eos&gt;&#x27;</span></span><br><span class="line"></span><br><span class="line">        self.special_chars = [self.pad,self.oov_char,self.bos,self.eos]</span><br><span class="line">        self.data = <span class="string">&quot;&quot;</span></span><br><span class="line">        self._words=<span class="built_in">set</span>()</span><br><span class="line">        self.tokenize=disabletokenize</span><br><span class="line">        self.tknzr = TweetTokenizer()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self,text</span>):</span><br><span class="line">        <span class="keyword">if</span>(self.tokenize):</span><br><span class="line">            self._words.update(text)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.data=self.tknzr.tokenize(text)</span><br><span class="line">            <span class="comment">#self.data=word_tokenize(text)</span></span><br><span class="line">            self._words.update(self.data)</span><br><span class="line">        self.w2i = &#123;w: (i + <span class="built_in">len</span>(self.special_chars)) <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(self._words)&#125;</span><br><span class="line"></span><br><span class="line">        self.i2w = &#123;i: w <span class="keyword">for</span> w, i <span class="keyword">in</span> self.w2i.items()&#125;</span><br><span class="line">        self.w2i[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>] = <span class="number">0</span></span><br><span class="line">        self.i2w[<span class="number">0</span>] = <span class="string">&#x27;&lt;pad&gt;&#x27;</span></span><br><span class="line">        self.w2i[<span class="string">&#x27;&lt;unk&gt;&#x27;</span>] = <span class="number">1</span></span><br><span class="line">        self.i2w[<span class="number">1</span>] = <span class="string">&#x27;&lt;unk&gt;&#x27;</span></span><br><span class="line">        self.w2i[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>] = <span class="number">2</span></span><br><span class="line">        self.i2w[<span class="number">2</span>] = <span class="string">&#x27;&lt;bos&gt;&#x27;</span></span><br><span class="line">        self.w2i[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>] = <span class="number">3</span></span><br><span class="line">        self.i2w[<span class="number">3</span>] = <span class="string">&#x27;&lt;eos&gt;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self,words</span>):</span><br><span class="line">        output=[]</span><br><span class="line">        <span class="keyword">if</span>(self.tokenize):</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#words=word_tokenize(words)</span></span><br><span class="line">            words=self.tknzr.tokenize(words)</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            <span class="comment">#辞書になし</span></span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.w2i:</span><br><span class="line">                index = self.w2i[self.oov_char]<span class="comment">#既存の&lt;unk&gt;を返す</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#辞書にあり</span></span><br><span class="line">                index = self.w2i[word]<span class="comment">#idを引っ張ってくる</span></span><br><span class="line">            output.append(index)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self,indexes</span>):<span class="comment">#使いどころないけど確認用</span></span><br><span class="line">        out=[]</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> indexes:</span><br><span class="line">            out.append(self.i2w[index])</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<p>trainデータを形態素に分解して単語を記録します。分解方法は<a target="_blank" rel="noopener" href="https://www.nltk.org/">Natural Language Toolkit</a>のtweet tokenizerを使用しました。ハッシュタグやリプライ、顔文字も認識して分解してくれます。(最初は一般的な形態素解析ツールで分解していたため、これらが過度に分解されてしまい、文字が増えてしまいました)</p>
<p>次に、train,testデータをそれぞれ登録済みの単語でid化していきます。未登録の単語はid&#x3D;1を振ります。今回は、学習データとしてtextとkeyを用いました。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keyword_voc.update(<span class="built_in">list</span>(keywords))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> raw_X_train:</span><br><span class="line">    text_voc.update(t[<span class="number">0</span>])</span><br><span class="line">maxlen=<span class="number">158</span></span><br></pre></td></tr></table></figure>

<h3 id="モデル設計部分"><a href="#モデル設計部分" class="headerlink" title="モデル設計部分"></a>モデル設計部分</h3><p>エンコードした文章をattentionし、その出力にkeyを結合してFFを通す事で出力を得ます。attention内部の、構造については、前回の翻訳と同様です。ただ、最初にも書いた通り、翻訳対象がないため、self-attentionのみを行っています。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionNN</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;&quot; 略 &quot;&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    def forward(self,source, key):</span></span><br><span class="line"><span class="string">        key=self.embedding_key(key)</span></span><br><span class="line"><span class="string">        mask_source = self.sequence_mask(source)</span></span><br><span class="line"><span class="string">        hs = self.Attantions(source, mask=mask_source)</span></span><br><span class="line"><span class="string">        a_out=hs.reshape(len(source),self.d_model*maxlen)#attention層出力</span></span><br><span class="line"><span class="string">        b_out =key</span></span><br><span class="line"><span class="string">        out = torch.cat([a_out,b_out],dim=1)#keywordを結合</span></span><br><span class="line"><span class="string">        out = self.linear_out(out)</span></span><br><span class="line"><span class="string">        out = self.activation_out(out)</span></span><br><span class="line"><span class="string">        return out</span></span><br></pre></td></tr></table></figure>

<h3 id="学習部分"><a href="#学習部分" class="headerlink" title="学習部分"></a>学習部分</h3><p>下画像のように、学習データが少しtarget&#x3D;0の方が多いため、ダウンサンプリングします。<br><img src="https://blog.gojiteji.com/images/kagglenl/hist.png" alt="trainデータの分布"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, y_train = rus.fit_sample(raw_X_train.reshape(-<span class="number">1</span>, <span class="number">2</span>) ,raw_y_train.reshape(-<span class="number">1</span>, <span class="number">1</span>) )</span><br></pre></td></tr></table></figure>

<p>また、test時に1&#x2F;4程度、新規の単語が増えるため、エポックごとに25% ランダな位置に&lt;unk&gt; 記号を含ませるようにしています。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">unks=<span class="number">0</span><span class="comment">#unkの量</span></span><br><span class="line">max_unks=(x_train.size - <span class="built_in">sum</span>(<span class="built_in">sum</span>(x_train==<span class="number">0</span>)))*<span class="number">0.25</span><span class="comment">#0以外の量</span></span><br><span class="line">max_row=x_train.shape[<span class="number">0</span>]</span><br><span class="line">max_colum=x_train.shape[<span class="number">1</span>]</span><br><span class="line">unks=[]</span><br><span class="line"><span class="keyword">while</span>(<span class="built_in">len</span>(unks)&lt;max_unks):</span><br><span class="line">    row=<span class="built_in">int</span>(np.random.rand()*max_row)</span><br><span class="line">    colum=<span class="built_in">int</span>(np.random.rand()*max_colum)</span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">not</span>(x_train[row,colum] == <span class="number">0</span>)):<span class="comment">#paddingじゃない</span></span><br><span class="line">        <span class="keyword">if</span>(<span class="keyword">not</span>(x_train[row,colum] == <span class="number">2</span>)):<span class="comment">#bosじゃない</span></span><br><span class="line">            <span class="keyword">if</span>(<span class="keyword">not</span>(x_train[row,colum] == <span class="number">3</span>)):<span class="comment">#eosじゃない</span></span><br><span class="line">                <span class="keyword">if</span>(<span class="keyword">not</span>(  [row,colum] <span class="keyword">in</span> unks)):<span class="comment">#既に置き換えていなければ</span></span><br><span class="line">                    x_train[row,colum]=<span class="number">1</span></span><br><span class="line">                    unks.append([row,colum])</span><br></pre></td></tr></table></figure>
<p>その他、学習部分に特別なものはありません。</p>
<h2 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h2><p>trainファイルの一部を検証に用いると、以下のような結果になりました。<br><img src="https://blog.gojiteji.com/images/kagglenl/loss.png" alt="損失関数の推移"><br><img src="https://blog.gojiteji.com/images/kagglenl/acc.png" alt="エポックに対する正答率の推移"></p>
<p>今回、各パラメータを</p>
<ul>
<li>エンコーダレイヤ数＝20</li>
<li>attentionヘッド数&#x3D;6</li>
<li>text埋め込み次元&#x3D;180</li>
<li>key埋め込み次元&#x3D;224</li>
<li>FFの次元&#x3D;190</li>
<li>学習率&#x3D;0.00001<br>と設定し、20エポックほど回しました。<br>提出結果、スコアは0.74539でした。まだまだ上がいるので、精進が必要そうです…</li>
</ul>
</div></article></div></main><footer><div class="paginator"><a href="/2020/03/22/kagglenl2/" class="prev">PREV</a><a href="/2020/02/28/ja2th/" class="next">NEXT</a></div><div class="copyright"><p>© 2020 - 2022 <a href="https://blog.gojtieji.com">gojiteji</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>