<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> (改)Real or Not? NLP with Disaster Tweets を解く · null</title><meta name="description" content="(改)Real or Not? NLP with Disaster Tweets を解く - gojiteji"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://blog.gojtieji.com/atom.xml"><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="null" type="application/atom+xml">
</head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/sunchongsheng" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/pinggod" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">(改)Real or Not? NLP with Disaster Tweets を解く</h1><div class="post-info">Mar 22, 2020</div><div class="post-content"><p><a target="_blank" rel="noopener" href="https://blog.gojiteji.com/2020/03/15/kagglenl/">前回の記事</a>を書いて以降、いくつか改善策を練った結果、精度が 74.539 % → 78.425 % に上がったため、その内容を書いておきます。</p>
<h2 id="改善点"><a href="#改善点" class="headerlink" title="改善点"></a>改善点</h2><p>今回の改善点は主に4つ</p>
<ol>
<li>人物系単語、地名系単語、@付き単語、リンクをまとめる</li>
<li>出現頻度の少ない単語は&lt;unk&gt;(辞書に存在しない単語)として扱う</li>
<li>モデルをCNNに変更</li>
<li>学習時の&lt;unk&gt;含有率をテストデータに合わせる</li>
</ol>
<span id="more"></span>
<p>trainデータを見てみると、@付き単語の種類数は2328に対して、@付き単語の出現回数は2759であり、ほとんどの出現回数は1回でした。もっとも使われている@付き単語は@YouTube(YouTube動画共有時についているやつ)で、82回でした。<br>今回、@付きアカウントは以下のように割り振りました。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;youtube&gt; : @YouTube</span><br><span class="line">&lt;news&gt; : ニュースアカウント</span><br><span class="line">&lt;social&gt; : 政府や組織のアカウント</span><br><span class="line">&lt;media&gt; : メディア雑誌系アカウント</span><br><span class="line">&lt;reply&gt; : others</span><br></pre></td></tr></table></figure>
<p>リンクは&lt;link&gt;、地名は&lt;location&gt;、人物名は、出現頻度の多い政治家・歌手等を除いて、&lt;person&gt;にまとめました。</p>
<p>出現回数が1度のみの単語は、おそらくtestデータ中にも出現しにくいと考えられるため全て&lt;unk&gt;にまとめました。(&lt;unk&gt;は元々、テストデータの単語が学習データ中に存在しない時に変換する文字)</p>
<p>今回まとめた単語は以上ですが、これら以外にも、出現頻度が少ない単語を、ある一定のグループ名にまとめると、精度が上がるのではないかと思います。<del>日が昇ってきたので前処理はこの辺で切り上げました</del></p>
<p>1.2.に関して、これら以外にも不要な記号等を削除した結果、25,000程度あった単語の種類が、6,000程度に減りました。</p>
<p>3.に関しては単純なCNNです。前回のモデルよりも、正解率が率が高くなりました。構造は以下のように、単純なものです。埋め込み次元は18次元としています。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,depth_source,depth_key,batch_size,maxlen,d_model,d_key,device=<span class="string">&#x27;cuda&#x27;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.device = device</span><br><span class="line">        self.embedding = nn.Embedding(depth_source,d_model, padding_idx=<span class="number">0</span>)</span><br><span class="line">        self.d_model=d_model</span><br><span class="line">        self.maxlen=maxlen</span><br><span class="line">        </span><br><span class="line">        self.c1 = nn.Conv2d(<span class="number">1</span>, <span class="number">8</span>, <span class="number">3</span>)</span><br><span class="line">        self.a1 = nn.ReLU()</span><br><span class="line">        self.p1 = nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">        self.d1 = nn.Dropout(p=<span class="number">0.4</span>)</span><br><span class="line"></span><br><span class="line">        self.c2 = nn.Conv2d(<span class="number">8</span>,<span class="number">16</span>, <span class="number">3</span>)</span><br><span class="line">        self.a2 = nn.ReLU()</span><br><span class="line">        self.p2 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.d2 = nn.Dropout(p=<span class="number">0.4</span>)</span><br><span class="line"></span><br><span class="line">        self.linear_out = nn.Linear(<span class="number">1824</span>, <span class="number">1</span>)</span><br><span class="line">        self.activation_out = nn.Sigmoid()</span><br><span class="line">        nn.init.xavier_normal_(self.linear_out.weight)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x, key</span>):</span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x=x.reshape(x.shape[<span class="number">0</span>],<span class="number">1</span>,x.shape[<span class="number">1</span>],x.shape[<span class="number">2</span>])</span><br><span class="line">        y=self.c1(x)</span><br><span class="line">        y=self.p1(y)</span><br><span class="line">        y=self.a1(y)</span><br><span class="line">        y=self.d1(y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        y=self.c2(y)</span><br><span class="line">        y=self.p2(y)</span><br><span class="line">        y=self.a2(y)</span><br><span class="line">        y=self.d2(y)</span><br><span class="line">        a_out=y.reshape(y.shape[<span class="number">0</span>],y.shape[<span class="number">1</span>]*y.shape[<span class="number">2</span>]*y.shape[<span class="number">3</span>])</span><br><span class="line">        out = self.linear_out(a_out)</span><br><span class="line">        out = self.activation_out(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<p>4.に関しては、学習データ中のunk含有率：0.04140461215932914　テストデータ中のunk含有率：6.409214826048494 %　この差分に近い、2.26%だけ&lt;unk&gt;をランダムで学習データに追加しました。</p>
<p>学習結果は以下の通りです。validationデータに対する正解率は0.77付近にいます。<br><img src="https://blog.gojiteji.com/images/kagglenl2/loss.png" alt="損失関数"><br><img src="https://blog.gojiteji.com/images/kagglenl2/acc.png" alt="正解率の推移"></p>
<p>提出結果は以下の通りです。<br><img src="https://blog.gojiteji.com/images/kagglenl2/test.png" alt="kaggleの提出結果"></p>
<h2 id="おわりに"><a href="#おわりに" class="headerlink" title="おわりに"></a>おわりに</h2><p>前回はデータをそのまま学習させていたため、今回はデータをいくつか前処理でまとめてみました。時間もかかって大変ですけど、データ数を減らすことで、結果的に学習時間を削減できましたので、もっと積極的にすべきですね..</p>
</div></article></div></main><footer><div class="paginator"><a href="/2020/04/27/mothman-current-report/" class="prev">PREV</a><a href="/2020/03/15/kagglenl/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2022 <a href="https://blog.gojtieji.com">gojiteji</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>