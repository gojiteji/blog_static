<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>gojiteji&#39;s Blog</title>
  
  
  <link href="https://blog.gojtieji.com/atom.xml" rel="self"/>
  
  <link href="https://blog.gojtieji.com/"/>
  <updated>2023-02-06T20:36:20.033Z</updated>
  <id>https://blog.gojtieji.com/</id>
  
  <author>
    <name>gojiteji</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CLIPにおける編集距離とユークリッド距離の考察と検証</title>
    <link href="https://blog.gojtieji.com/2023/02/06/CLIPdistance/"/>
    <id>https://blog.gojtieji.com/2023/02/06/CLIPdistance/</id>
    <published>2023-02-06T11:24:13.000Z</published>
    <updated>2023-02-06T20:36:20.033Z</updated>
    
    <content type="html"><![CDATA[<h2 id="イントロ"><a href="#イントロ" class="headerlink" title="イントロ"></a>イントロ</h2><center><img src="https://blog.gojiteji.com/images/clip/applesign.png"/ width=320></center><center><img src="https://blog.gojiteji.com/images/clip/apple3.png"/ width=320></center><p>Stable diffusion[1]では，<code>A road sign with the word &quot;apple&quot;</code> と入力すれば，上の画像のように，文字を画像化することが可能です．<br>一方で，下の画像のように「意味に対応する文字っぽいもの」がそれとなく表示されるものの，果たして本当に”文字”として学習しているのか？という疑問を持ったため，簡単に実験してみました．</p><p>Character-Aware Models Improve Visual Text Rendering.[3]では，画像生成とその画像文字の質に関する精度が大規模に実験されています．中でも，Stable Diffusionや，Imagen[4]では，文字画像生成にはにはミスが多く存在し，画像生成にバイトレベルTransformer(ByT5[5])を用いると，文字の生成クオリティが向上したとの結果が報告されています．</p><p>今回は，文字列とトークンがどれだけ対応しているかを，トークンの文字列の距離と，トークンの埋め込みの距離を比較してみます．</p><span id="more"></span><h2 id="実験"><a href="#実験" class="headerlink" title="実験"></a>実験</h2><p>今回の目標はトークンの文字情報の編集距離とトークンの隠れベクトルのユークリッド距離が，どの程度異なるのか，を調べることです．<br>文字列情報が近いことが重要であり，トークンの意味が類似する必要はないため，cos類似度ではなく，簡易な距離の一例としてユークリッド距離を比較しました．</p><p>今回使うモデルは，Stable Diffusionに使われているCLIPのテキストエンコーダです．CLIPの語彙に対して．全ての語彙から2組の組み合わせに対して距離を計算してどんな関係になっているのかをみてみます．<br>なるべく埋め込みに文字情報を持たせたいので，promptは，<code>A road sign with the word &quot;&#123;単語&#125;&quot;</code>と入力し，単語部分のみを埋め込みを利用します．</p><p>また，細かいですが，以下の条件を設定します．</p><ul><li>文字列種類の90%以上が異なる文字からなる単語同士の比較は，本題の趣旨から逸れるため計算処理から外しています．</li><li>3文字以上長さが異なる文字からなる単語同士の比較は，本題の趣旨から逸れるため計算処理から外しています．</li><li>計算コストや生起率から10文字以上のトークンは計算に含めていません．</li><li>複雑すぎる文字の編集も，本誌の趣旨がら逸れるため，編集距離は3以下のものを計算しています．</li><li>英語以外を含むトークンは削除</li></ul><p>条件をパスしたトークン対の例</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">acci acia</span><br><span class="line">acci attic</span><br><span class="line">acci aci</span><br><span class="line">acci rica</span><br><span class="line">acci chai</span><br><span class="line">acci cia</span><br><span class="line">book bloke</span><br><span class="line">book books</span><br><span class="line">book bok</span><br><span class="line">book brook</span><br><span class="line">book ebook</span><br></pre></td></tr></table></figure><p>人間である自分からすると，上の例の単語は意味は異なっても，なんとなく文字列が近いことは直感的にわかります．</p><p>実験コードはGitHubにあげています．修正点等ありましたら，PRやissueで連絡ください．<br><a href="https://github.com/gojiteji/CLIPdistance">https://github.com/gojiteji/CLIPdistance</a></p><h2 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h2><p>結果をまとめると，以下の通りです．</p><table><thead><tr><th></th><th>編集距離&#x3D;1</th><th>編集距離&#x3D;2</th><th>編集距離&#x3D;3</th><th>ランダムな10k単語ペア</th></tr></thead><tbody><tr><td>ユークリッド距離</td><td>24.1428</td><td>24.1462</td><td>24.1235</td><td>23.7973</td></tr><tr><td>分散</td><td>2.2810</td><td>2.3037</td><td>2.3306</td><td>-</td></tr></tbody></table><p>編集距離が近いからと言って，ユークリッド距離が近い，ということはなさそうです．<br>一方で，ランダムなトークンのペアに比べると，少しユークリッド距離が伸びたように見えます(なぜ?)．</p><h2 id="終わりに"><a href="#終わりに" class="headerlink" title="終わりに"></a>終わりに</h2><p>編集距離と言っても，削除や挿入と，置換は埋め込みベクトルに対してかなり性質が異なる気がするので，それら個別の操作に対する埋め込みの変化も気になるところです．またベクトル間の距離ももっとトークンの文字情報がうまく扱われている情報がある気もします．</p><p>CLIP-largeモデルでは流石に時間がかかりすぎたので，でかいモデルでもどうなるか，どなたか検証してくれることに期待です．</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2021. High-Resolution Image Synthesis with Latent Diffusion Models.</p><p>[2] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. arXiv.</p><p>[3] Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, R. J. Mical, Mohammad Norouzi, and Noah Constant. 2022. Character-Aware Models Improve Visual Text Rendering. arXiv.</p><p>[4] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.</p><p>[5] Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2022. ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models. Transactions of the Association for Computational Linguistics, 10:291–306.</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;イントロ&quot;&gt;&lt;a href=&quot;#イントロ&quot; class=&quot;headerlink&quot; title=&quot;イントロ&quot;&gt;&lt;/a&gt;イントロ&lt;/h2&gt;&lt;center&gt;&lt;img src=&quot;https://blog.gojiteji.com/images/clip/applesign.png&quot;/ width=320&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&quot;https://blog.gojiteji.com/images/clip/apple3.png&quot;/ width=320&gt;&lt;/center&gt;

&lt;p&gt;Stable diffusion[1]では，&lt;code&gt;A road sign with the word &amp;quot;apple&amp;quot;&lt;/code&gt; と入力すれば，上の画像のように，文字を画像化することが可能です．&lt;br&gt;一方で，下の画像のように「意味に対応する文字っぽいもの」がそれとなく表示されるものの，果たして本当に”文字”として学習しているのか？という疑問を持ったため，簡単に実験してみました．&lt;/p&gt;
&lt;p&gt;Character-Aware Models Improve Visual Text Rendering.[3]では，画像生成とその画像文字の質に関する精度が大規模に実験されています．中でも，Stable Diffusionや，Imagen[4]では，文字画像生成にはにはミスが多く存在し，画像生成にバイトレベルTransformer(ByT5[5])を用いると，文字の生成クオリティが向上したとの結果が報告されています．&lt;/p&gt;
&lt;p&gt;今回は，文字列とトークンがどれだけ対応しているかを，トークンの文字列の距離と，トークンの埋め込みの距離を比較してみます．&lt;/p&gt;</summary>
    
    
    
    
    <category term="NLP" scheme="https://blog.gojtieji.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>NAGIシステムの概要</title>
    <link href="https://blog.gojtieji.com/2022/12/21/nagi-system/"/>
    <id>https://blog.gojtieji.com/2022/12/21/nagi-system/</id>
    <published>2022-12-20T15:00:00.000Z</published>
    <updated>2022-12-21T10:36:47.255Z</updated>
    
    <content type="html"><![CDATA[<p><strong>これは<a href="https://qiita.com/advent-calendar/2022/naist">NAISTアドベントカレンダー2022</a> 21日目の投稿です。</strong></p><p>先日HuggingFace Spacesにて，3つのAIによる決議システム「<a href="https://huggingface.co/spaces/gojiteji/NAGISystem">NAGI System</a>」を公開しました．その仕組みを簡単に紹介します．</p><span id="more"></span><h2 id="仕組み"><a href="#仕組み" class="headerlink" title="仕組み"></a>仕組み</h2><p>promptという手法を使っています．近年の機械学習モデルを特定のタスクに当てるとなると，1. 事前学習と2. fine-tuningの二つのステップが必要です．一方で，promptというのは，1. は同じですが，その次にfine-tuningを行いません．じゃあどうしているかというと，事前学習のタスクに近い形でタスク情報を入力してやれば，fine-tuningせずとも，特定タスクを解く性能は持ってるよね，といった感じで解いています．<br>具体的には，次のようなタスクを解いています．</p><h2 id="BERTの場合"><a href="#BERTの場合" class="headerlink" title="BERTの場合"></a>BERTの場合</h2><p>BERTは事前学習で入力文章の<code>[MASK]</code>部分を埋めるタスクを解いています．<br>なので，promptでは2022&#x2F;12&#x2F;19時点で以下のような入力をBERTに与えています．</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;MELCHIORは科学者としての人格を持っています。人間とMELCHIORの対話です。人間「&quot;</span>+sue+<span class="string">&quot;。承認 か 否定 のどちらかで答えてください。」&quot;</span>+<span class="string">&quot;MELCHIOR 「[MASK]」&quot;</span></span><br></pre></td></tr></table></figure><p><a href="https://huggingface.co/spaces/gojiteji/NAGISystem/blob/main/app.py#L18">ソースコード</a></p><h2 id="GPT2の場合"><a href="#GPT2の場合" class="headerlink" title="GPT2の場合"></a>GPT2の場合</h2><p>GPT2は事前学習で入力文章の次単語を予測するタスクを解いています．<br>なので，promptでは2022&#x2F;12&#x2F;19時点で以下のような入力をGPT2に与えています．</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;CASPERは女としての人格を持っています。人間とCASPERの対話です。人間「&#x27;</span>+sue+<span class="string">&#x27;。承認 か 否定 のどちらかで答えてください。」&#x27;</span>+<span class="string">&quot;CASPER 「&quot;</span></span><br></pre></td></tr></table></figure><p><a href="https://huggingface.co/spaces/gojiteji/NAGISystem/blob/main/app.py#L27">ソースコード</a></p><h2 id="mT5の場合可決割合の調節のためmBERTに変更"><a href="#mT5の場合可決割合の調節のためmBERTに変更" class="headerlink" title="mT5の場合可決割合の調節のためmBERTに変更"></a><del>mT5の場合</del>可決割合の調節のためmBERTに変更</h2><p><del>mT5は事前学習で<code>&lt;X&gt;</code>や<code>&lt;Y&gt;</code>でマスクされた文を予測するタスクを解いています．</del><br><del>なので，promptでは2022&#x2F;12&#x2F;19時点で以下のような入力をmT5に与えています．</del></p><p><del>‘BALTHASARは母としての人格としての人格を持っています。人間とBALTHASARの対話です。人間「’+sue+’。承認 か 否定 のどちらかで答えてください。」’+”BALTHASAR 「<X>」”</del></p><p><del><a href="https://huggingface.co/spaces/gojiteji/NAGISystem/blob/main/app.py#L30">ソースコード</a></del></p><h2 id="出力に関して"><a href="#出力に関して" class="headerlink" title="出力に関して"></a>出力に関して</h2><p>もちろん，これらの回答が必ず「承認」か「否定」かになるわけではないです．そこで，「承認」のスコアと「否定」のスコアを比較して結果を出力しています．おそらくですが，「承認」と「否定」を事前学習で学習で対になるように学習できていないので，現状学習データに多く含まれていた「否定」が多く出る状況になっているのだと思います．</p><p>こういったことが簡単に実装できる，AIの民主化を大きく進めてくれたHuggingFace社には感謝しかないですね🤗</p><h2 id="終わりに"><a href="#終わりに" class="headerlink" title="終わりに"></a>終わりに</h2><p>近年のStable Diffusion関連の話題で聞いたことある単語が出てきたって人もいるのではないでしょうか．少しでもAI・機械学習分野の中身に興味を持っていただけたら幸いです！</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing.<br><a href="https://arxiv.org/abs/2107.13586">https://arxiv.org/abs/2107.13586</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;これは&lt;a href=&quot;https://qiita.com/advent-calendar/2022/naist&quot;&gt;NAISTアドベントカレンダー2022&lt;/a&gt; 21日目の投稿です。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;先日HuggingFace Spacesにて，3つのAIによる決議システム「&lt;a href=&quot;https://huggingface.co/spaces/gojiteji/NAGISystem&quot;&gt;NAGI System&lt;/a&gt;」を公開しました．その仕組みを簡単に紹介します．&lt;/p&gt;</summary>
    
    
    
    
    <category term="NLP" scheme="https://blog.gojtieji.com/tags/NLP/"/>
    
    <category term="AI" scheme="https://blog.gojtieji.com/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>言語モデルの語彙の違いを見てみる</title>
    <link href="https://blog.gojtieji.com/2022/12/14/languagemodels/"/>
    <id>https://blog.gojtieji.com/2022/12/14/languagemodels/</id>
    <published>2022-12-13T15:00:00.000Z</published>
    <updated>2022-12-13T15:25:57.517Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>これは<a href="https://qiita.com/advent-calendar/2022/naist">NAISTアドベントカレンダー2022</a> 1日目の代理投稿です。</strong></p></blockquote><p>「語彙力がない」「ボキャ貧」なんて言葉を人間は使いますが，AIにもそういった特徴はあるのでしょうか？少し気になったので検証してみました．やり方は簡単で，言語モデルごとの語彙の集合を用いてベン図を描きました．</p><p>ソースコード：<a href="https://github.com/gojiteji/AI_vocab_comparison/blob/main/Vocab_comparison.ipynb">https://github.com/gojiteji/AI_vocab_comparison/blob/main/Vocab_comparison.ipynb</a></p><p>注意：<br>トークン化手法ごとにprefixが違う場合あります．異なるトークン化手法を用いているものは，prefixは全て削除しました．従って，後続トークンとしてのback(ex. feedback)と，先頭トークンとbackが同一トークンとして扱われます．また，extra idや言語コード，special tokenは削除しておりません．</p><span id="more"></span><h2 id="T5-vs-mT5"><a href="#T5-vs-mT5" class="headerlink" title="T5 vs mT5"></a>T5 vs mT5</h2><table><thead><tr><th>モデル名</th><th>トークン化手法</th><th>データセット</th><th>リンク</th></tr></thead><tbody><tr><td>T5</td><td>SentencePiece</td><td>C4</td><td><a href="https://huggingface.co/t5-small">https://huggingface.co/t5-small</a></td></tr><tr><td>mT5</td><td>SentencePiece</td><td>mC4</td><td><a href="https://huggingface.co/google/mt5-small">https://huggingface.co/google/mt5-small</a></td></tr></tbody></table><p><img src="https://blog.gojiteji.com/images/vocab/T5vsmT5.png" width="50%"></img></p><p>データセットがC4より多いmC4を使っているだけあって，mT5の方が語彙サイズがとても大きいことがわかります．</p><h2 id="mBART25-vs-mBART50-vs-mT5"><a href="#mBART25-vs-mBART50-vs-mT5" class="headerlink" title="mBART25 vs mBART50 vs mT5"></a>mBART25 vs mBART50 vs mT5</h2><table><thead><tr><th>モデル名</th><th>トークン化手法</th><th>データセット</th><th>リンク</th></tr></thead><tbody><tr><td>mBART25</td><td>SentencePiece</td><td>Common Crawl</td><td><a href="https://huggingface.co/facebook/facebook/mbart-large-cc25">https://huggingface.co/facebook/facebook/mbart-large-cc25</a></td></tr><tr><td>mBART50</td><td>SentencePiece</td><td>Same as mBART25</td><td><a href="https://huggingface.co/google/facebook/mbart-large-50">https://huggingface.co/google/facebook/mbart-large-50</a></td></tr><tr><td>mT5</td><td>SentencePiece</td><td>mC4</td><td><a href="https://huggingface.co/google/mt5-small">https://huggingface.co/google/mt5-small</a></td></tr></tbody></table><p><img src="https://blog.gojiteji.com/images/vocab/mBART25vsmBART50vsmT5.png" width="50%"></img></p><p>データセットはいずれも CommonCrawl で，mBART25とmBART50はトークナイザは言語コードの分が違うだけのようですね<br>ちなみに，異なる27個の語彙は以下の通りです．</p><blockquote><p>‘af_ZA’,’az_AZ’,’bn_IN’,’fa_IR’,’gl_ES’,’he_IL’,’hr_HR’,’id_ID’,’ka_GE’,’km_KH’,’mk_MK’,’ml_IN’,’mn_MN’,’mr_IN’,’pl_PL’,’ps_AF’,’pt_XX’,’sl_SI’,’sv_SE’,’sw_KE’,’ta_IN’,’te_IN’,’th_TH’,’tl_XX’,’uk_UA’,’ur_PK’,’xh_ZA’</p></blockquote><h2 id="GPT-J-vs-GPT-2-x2F-3"><a href="#GPT-J-vs-GPT-2-x2F-3" class="headerlink" title="GPT-J vs GPT-2&#x2F;3"></a>GPT-J vs GPT-2&#x2F;3</h2><p>これは<a href="https://github.com/kingoflolz/mesh-transformer-jax/">GPT-JのGitHubに書いてありますが</a>，GPT-2&#x2F;3と同じtokenizer(Byte Pair Encoding)を使っているそうです．(以下略)</p><h2 id="GPT2-japanese-vs-BERT-japanese-vs-T5-japanese"><a href="#GPT2-japanese-vs-BERT-japanese-vs-T5-japanese" class="headerlink" title="GPT2-japanese vs BERT-japanese vs T5-japanese"></a>GPT2-japanese vs BERT-japanese vs T5-japanese</h2><table><thead><tr><th>モデル名</th><th>トークン化手法</th><th>データセット</th><th>リンク</th></tr></thead><tbody><tr><td>GPT2-japanese</td><td>SentencePiece</td><td>Wikipedia(ja)</td><td><a href="https://huggingface.co/rinna/japanese-gpt2-medium">https://huggingface.co/rinna/japanese-gpt2-medium</a></td></tr><tr><td>BERT-japanese</td><td>WordPiece</td><td>IPA dictionary</td><td><a href="https://huggingface.co/cl-tohoku/bert-base-japanese">https://huggingface.co/cl-tohoku/bert-base-japanese</a></td></tr><tr><td>T5-japanese</td><td>SentencePiece</td><td>Wikipedia(ja),OSCAR(ja),CC-100(ja)</td><td><a href="https://huggingface.co/sonoisa/t5-base-japanese">https://huggingface.co/sonoisa/t5-base-japanese</a></td></tr></tbody></table><p><img src="https://blog.gojiteji.com/images/vocab/GPTjavsBERTjavsT5ja.png" width="50%"></img></p><p>同じ日本語wikipediaをトークナイザの学習に使っているため，GPT2-japaneseとT5-japaneseの共通部分が大きいですね．微妙に異なる部分がるのは，special tokensやdumpの時期の違いによるものでしょうか．（何かご存知の方いらっしゃったらご教授ください）</p><h2 id="最後に"><a href="#最後に" class="headerlink" title="最後に"></a>最後に</h2><p>モデルとトークナイザは学習が独立していることが多いため，トークナイザが単体でどのような挙動をしているかを知っておくことも重要なのだと思いました．この辺もちゃんと理解して「ニューラルネットの気持ち」になりたいですね．</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;これは&lt;a href=&quot;https://qiita.com/advent-calendar/2022/naist&quot;&gt;NAISTアドベントカレンダー2022&lt;/a&gt; 1日目の代理投稿です。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;「語彙力がない」「ボキャ貧」なんて言葉を人間は使いますが，AIにもそういった特徴はあるのでしょうか？少し気になったので検証してみました．やり方は簡単で，言語モデルごとの語彙の集合を用いてベン図を描きました．&lt;/p&gt;
&lt;p&gt;ソースコード：&lt;a href=&quot;https://github.com/gojiteji/AI_vocab_comparison/blob/main/Vocab_comparison.ipynb&quot;&gt;https://github.com/gojiteji/AI_vocab_comparison/blob/main/Vocab_comparison.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;注意：&lt;br&gt;トークン化手法ごとにprefixが違う場合あります．異なるトークン化手法を用いているものは，prefixは全て削除しました．従って，後続トークンとしてのback(ex. feedback)と，先頭トークンとbackが同一トークンとして扱われます．また，extra idや言語コード，special tokenは削除しておりません．&lt;/p&gt;</summary>
    
    
    
    
    <category term="NLP" scheme="https://blog.gojtieji.com/tags/NLP/"/>
    
    <category term="AI" scheme="https://blog.gojtieji.com/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion モデルはQRコードの符号化アルゴリズムを模倣できるか？</title>
    <link href="https://blog.gojtieji.com/2022/12/02/adventcalendar2022/"/>
    <id>https://blog.gojtieji.com/2022/12/02/adventcalendar2022/</id>
    <published>2022-12-01T15:00:00.000Z</published>
    <updated>2022-12-01T22:13:27.718Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>これは<a href="https://qiita.com/advent-calendar/2022/naist">NAISTアドベントカレンダー2022</a>二日目の投稿です。</strong></p></blockquote><meta property="og:image" content="https://blog.gojiteji.com/images/2022adcale/noise.png" /><img src="https://blog.gojiteji.com/images/2022adcale/noise.png" width="80%"></img><h2 id="イントロ"><a href="#イントロ" class="headerlink" title="イントロ"></a>イントロ</h2><p>2022年を振り返ると、Stable diffusionを用いたモデルが研究者からイラストレーターまで、その生成精度の高さから様々なインパクトを与えた年でした。特に、Text-to-Imageである点、文字で条件付けができることは、人間の考えとAIの表現のコミュニケーションエラーが格段に縮まったように思います。</p><p>突然ですが、1990年代に生まれたText-to-Imageといえば、思い当たるものが一つありませんか？</p><span id="more"></span><p><strong>そう、<a href="https://www.denso-wave.com/ja/technology/vol1.html">QRコード</a>です！</strong></p><p>QRコードは頑張れば、<a href="https://www.nayuki.io/page/creating-a-qr-code-step-by-step">人間でも計算して描き上げること</a>は可能です。</p><p>では、もしかしたらAIもQRコード符号化アルゴリズムを学習することができるのではないでしょうか？（<del>論理の飛躍</del>）</p><p>言い換えれば、QRコードの符号化アルゴリズムはdiffusionモデルで模倣可能なのか？というのを、簡単な実験ではありますが、やってみました。</p><h2 id="データセット生成"><a href="#データセット生成" class="headerlink" title="データセット生成"></a>データセット生成</h2><p>今回、<a href="https://huggingface.co/datasets/sst2">sst2</a>の英語データセットから、pythonの<a href="https://pypi.org/project/qrcode/">qrcodeライブラリ</a>でqrコードを生成し、<a href="https://huggingface.co/datasets/gojiteji/QRsst2">QRsst2</a>というデータセットを作成しました。</p><h2 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a>Fine-tuning</h2><p>事前学習モデルはLudwig Maximilian University of Munich　CompVis Groupが公開している<a href="https://huggingface.co/CompVis/stable-diffusion-v-1-4-original">stable-diffusion-v-1-4-original</a>を使いました。</p><p>学習はGoogle Colab Pro+を契約して、A100を使って3エポック学習させました。</p><h2 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h2><p>promptによるQRコード生成は以下から実行できます。torchやcudaのバージョン周りでエラーが出やすいので、必要に応じてバージョンを確認して合うものを入れ直すなどしてください。（Freeプランでの動作は検証していません。）<br><a href="https://github.com/gojiteji/text2QR/blob/main/samplecode.ipynb">https://github.com/gojiteji/text2QR/blob/main/samplecode.ipynb</a><br>それっぽいものは生成できますが、スタイル変換として事前学習の影響が出ているのと、過学習気味のように見えます。</p><p>prompt: <code>&#39;How are you?</code></p><p><img src="https://blog.gojiteji.com/images/2022adcale/qr1.png" width="50%"></img></p><p>prompt: <code>https://google.com</code>(2枚生成)<br><img src="https://blog.gojiteji.com/images/2022adcale/qr2.png" width="50%"></img></p><p>prompt: <code>Stable diffusion is the best technology in 2022.</code>(16枚生成)<br><img src="https://blog.gojiteji.com/images/2022adcale/qr3.png" width="100%"></img></p><p>たまに、iPhoneのカメラアプリでQRコードであると判定してくれるものを生成してくれます。</p><blockquote class="twitter-tweet"><p lang="ja" dir="ltr">これはiPhoneはQRコードだとは認識してくれるがなんも出ない <a href="https://t.co/K6AmlNW6Du">pic.twitter.com/K6AmlNW6Du</a></p>&mdash; Koki Tanaka⚡️ (@gojiteji) <a href="https://twitter.com/gojiteji/status/1594629463891136513?ref_src=twsrc%5Etfw">November 21, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><p>もし何か読み取れるものが生成されたら教えてください。</p><h2 id="所感"><a href="#所感" class="headerlink" title="所感"></a>所感</h2><p>だいぶとゴリ押しで学習させたので、トークナイズをcharacter&#x2F;byte-levelにするなど、もっと”符号化”のプロセスを学習させるやりようがあるように思います。<br>あと、Fine-tuningの時にpromptに<code>QRcode: </code>を追加したらよかったかな…🤔?</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>テクノロジー&#x2F;QRコード開発 - Denso WAVE<br> <a href="https://www.denso-wave.com/ja/technology/vol1.html">https://www.denso-wave.com/ja/technology/vol1.html</a></li><li>justinpinkney&#x2F;stable-diffusion - GitHub<br>  <a href="https://github.com/justinpinkney/stable-diffusion">https://github.com/justinpinkney/stable-diffusion</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;これは&lt;a href=&quot;https://qiita.com/advent-calendar/2022/naist&quot;&gt;NAISTアドベントカレンダー2022&lt;/a&gt;二日目の投稿です。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;meta property=&quot;og:image&quot; content=&quot;https://blog.gojiteji.com/images/2022adcale/noise.png&quot; /&gt;
&lt;img src=&quot;https://blog.gojiteji.com/images/2022adcale/noise.png&quot; width=&quot;80%&quot;&gt;&lt;/img&gt;


&lt;h2 id=&quot;イントロ&quot;&gt;&lt;a href=&quot;#イントロ&quot; class=&quot;headerlink&quot; title=&quot;イントロ&quot;&gt;&lt;/a&gt;イントロ&lt;/h2&gt;&lt;p&gt;2022年を振り返ると、Stable diffusionを用いたモデルが研究者からイラストレーターまで、その生成精度の高さから様々なインパクトを与えた年でした。特に、Text-to-Imageである点、文字で条件付けができることは、人間の考えとAIの表現のコミュニケーションエラーが格段に縮まったように思います。&lt;/p&gt;
&lt;p&gt;突然ですが、1990年代に生まれたText-to-Imageといえば、思い当たるものが一つありませんか？&lt;/p&gt;</summary>
    
    
    
    
    <category term="AI" scheme="https://blog.gojtieji.com/tags/AI/"/>
    
    <category term="Text2Image" scheme="https://blog.gojtieji.com/tags/Text2Image/"/>
    
  </entry>
  
  <entry>
    <title>Padding process in HuggingFace transformer models</title>
    <link href="https://blog.gojtieji.com/2022/11/18/huggingface-how-padding-works/"/>
    <id>https://blog.gojtieji.com/2022/11/18/huggingface-how-padding-works/</id>
    <published>2022-11-18T07:42:35.000Z</published>
    <updated>2022-11-18T09:11:00.201Z</updated>
    
    <content type="html"><![CDATA[<p> While reading the code for Hugging Face’s T5, I felt uncomfortable with the processing around the paddings, so I examined them below.</p><span id="more"></span><p>HuggingFace’s attention layer implementation codes are here:<br><a href="https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L333">https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L333</a></p><h2 id="round-off-error-utilization-How-the-padding-columns-in-attention-matrix-is-converted-to-0"><a href="#round-off-error-utilization-How-the-padding-columns-in-attention-matrix-is-converted-to-0" class="headerlink" title="[round-off error utilization] How the padding columns in attention matrix is converted to 0?"></a>[round-off error utilization] How the padding columns in attention matrix is converted to 0?</h2><p>In the calculation of getting new <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D449" xlink:href="#MJX-1-TEX-I-1D449"></use></g></g></g></svg></mjx-container> in the attention layer, paddings’ attention must be 0 like <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.812ex;" xmlns="http://www.w3.org/2000/svg" width="11.783ex" height="2.432ex" role="img" focusable="false" viewBox="0 -716 5208.1 1075.1" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-1-TEX-N-70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z"></path><path id="MJX-1-TEX-N-61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z"></path><path id="MJX-1-TEX-N-64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z"></path><path id="MJX-1-TEX-N-69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z"></path><path id="MJX-1-TEX-N-6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path id="MJX-1-TEX-N-67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D434" xlink:href="#MJX-1-TEX-I-1D434"></use></g><g data-mml-node="TeXAtom" transform="translate(783,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="msub" transform="translate(623,0)"><g data-mml-node="mi"><use data-c="1D457" xlink:href="#MJX-1-TEX-I-1D457"></use></g><g data-mml-node="TeXAtom" transform="translate(445,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="70" xlink:href="#MJX-1-TEX-N-70"></use><use data-c="61" xlink:href="#MJX-1-TEX-N-61" transform="translate(556,0)"></use><use data-c="64" xlink:href="#MJX-1-TEX-N-64" transform="translate(1056,0)"></use><use data-c="64" xlink:href="#MJX-1-TEX-N-64" transform="translate(1612,0)"></use><use data-c="69" xlink:href="#MJX-1-TEX-N-69" transform="translate(2168,0)"></use><use data-c="6E" xlink:href="#MJX-1-TEX-N-6E" transform="translate(2446,0)"></use><use data-c="67" xlink:href="#MJX-1-TEX-N-67" transform="translate(3002,0)"></use></g></g></g></g></g><g data-mml-node="mo" transform="translate(3652.3,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(4708.1,0)"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g></g></g></svg></mjx-container> , but I cannot find explicitly substituting 0 in the codes.</p><p>This is implemented by utilizing round-off error.<br>As you know, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="23.521ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 10396.2 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-N-73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path><path id="MJX-1-TEX-N-6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z"></path><path id="MJX-1-TEX-N-66" d="M273 0Q255 3 146 3Q43 3 34 0H26V46H42Q70 46 91 49Q99 52 103 60Q104 62 104 224V385H33V431H104V497L105 564L107 574Q126 639 171 668T266 704Q267 704 275 704T289 705Q330 702 351 679T372 627Q372 604 358 590T321 576T284 590T270 627Q270 647 288 667H284Q280 668 273 668Q245 668 223 647T189 592Q183 572 182 497V431H293V385H185V225Q185 63 186 61T189 57T194 54T199 51T206 49T213 48T222 47T231 47T241 46T251 46H282V0H273Z"></path><path id="MJX-1-TEX-N-74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z"></path><path id="MJX-1-TEX-N-6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path id="MJX-1-TEX-N-61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z"></path><path id="MJX-1-TEX-N-78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-N-221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="73" xlink:href="#MJX-1-TEX-N-73"></use><use data-c="6F" xlink:href="#MJX-1-TEX-N-6F" transform="translate(394,0)"></use><use data-c="66" xlink:href="#MJX-1-TEX-N-66" transform="translate(894,0)"></use><use data-c="74" xlink:href="#MJX-1-TEX-N-74" transform="translate(1200,0)"></use><use data-c="6D" xlink:href="#MJX-1-TEX-N-6D" transform="translate(1589,0)"></use><use data-c="61" xlink:href="#MJX-1-TEX-N-61" transform="translate(2422,0)"></use><use data-c="78" xlink:href="#MJX-1-TEX-N-78" transform="translate(2922,0)"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(3450,0)"><g data-mml-node="mo"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mo" transform="translate(389,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(833.7,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(1278.3,0)"><use data-c="2212" xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(2056.3,0)"><use data-c="221E" xlink:href="#MJX-1-TEX-N-221E"></use></g><g data-mml-node="mo" transform="translate(3056.3,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g></g><g data-mml-node="mo" transform="translate(7173.1,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(8228.9,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mo" transform="translate(8617.9,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mo" transform="translate(9062.6,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(9507.2,0)"><use data-c="30" xlink:href="#MJX-1-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(10007.2,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></g></svg></mjx-container> .<br>The variable <code>mask</code> is using minimum value of the type. For example, pytorch’s <code>float</code> is shown below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.finfo(torch.<span class="built_in">float</span>).<span class="built_in">min</span></span><br><span class="line">-<span class="number">3.4028234663852886e+38</span></span><br></pre></td></tr></table></figure><p>This is very small number, but it’s not <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="4.023ex" height="1.505ex" role="img" focusable="false" viewBox="0 -583 1778 665" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-N-221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><use data-c="2212" xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(778,0)"><use data-c="221E" xlink:href="#MJX-1-TEX-N-221E"></use></g></g></g></svg></mjx-container> . Then do some errors remind in a calculation result?</p><p>The answer is no. The softmax calculation rounds this minimum to zero. Thus, there is no need to compute exactly minus infinity.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>vec=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,torch.finfo(torch.<span class="built_in">float</span>).<span class="built_in">min</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.softmax(vec,<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">0.2689</span>, <span class="number">0.7311</span>, <span class="number">0.0000</span>])</span><br></pre></td></tr></table></figure><p>The code is here:</p><ul><li>type’s minimum value for mask:<br>  <a href="https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L529">https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L529</a></li><li>softmax round-off error utilization<br>  <a href="https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L539">https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L539</a></li></ul><h2 id="loss-of-trailing-digits-utilization-How-is-the-padding-mask-applied-to-the-original-attention-matrix"><a href="#loss-of-trailing-digits-utilization-How-is-the-padding-mask-applied-to-the-original-attention-matrix" class="headerlink" title="[loss of trailing digits utilization] How is the padding mask applied to the original attention matrix?"></a>[loss of trailing digits utilization] How is the padding mask applied to the original attention matrix?</h2><p> As I mentioned above, the minimum value of the type is assigned in the dimension of the padding in the variable <code>mask</code>. Then, <a href="https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L529">the mask is added to position bias</a>, which is <a href="https://github.com/huggingface/transformers/blob/df56c843be370bd73d9ce4e597d4c7f55e4e41d9/src/transformers/models/t5/modeling_t5.py#L538">added to the attention matrix later</a>.</p><p> NOTE: <code>mask</code> is a vec here. Adding a vec to the matrix adds a vec to all rows.<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros(<span class="number">3</span>,<span class="number">3</span>)+torch.ones(<span class="number">3</span>)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure></p><p>But is it safe to <strong>add</strong> the minimum value (not <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="4.023ex" height="1.505ex" role="img" focusable="false" viewBox="0 -583 1778 665" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-N-221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><use data-c="2212" xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(778,0)"><use data-c="221E" xlink:href="#MJX-1-TEX-N-221E"></use></g></g></g></svg></mjx-container> ) instead of assigning it?<br>The answer is yes. This is a utilization of loss of trailing digits.<br>When calculating large and small absolute numbers in the floating point exhibition, python ignores the smaller one. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.finfo(torch.<span class="built_in">float</span>).<span class="built_in">min</span> == torch.finfo(torch.<span class="built_in">float</span>).<span class="built_in">min</span>+<span class="number">1e3</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><p><code>position_bias</code> is so small in absolute value compared to the minimum value of the type that adding them together does not change the fact that they are equivalent to -infinity.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt; While reading the code for Hugging Face’s T5, I felt uncomfortable with the processing around the paddings, so I examined them below.&lt;/p&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://blog.gojtieji.com/tags/ML/"/>
    
    <category term="HuggingFace" scheme="https://blog.gojtieji.com/tags/HuggingFace/"/>
    
  </entry>
  
  <entry>
    <title>nn.Module を読んでみた</title>
    <link href="https://blog.gojtieji.com/2022/10/29/module/"/>
    <id>https://blog.gojtieji.com/2022/10/29/module/</id>
    <published>2022-10-29T07:28:02.000Z</published>
    <updated>2022-10-30T08:44:35.911Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">この資料はラボ内自主勉強会に向けて書いたものです。間違い等ありましたら教えてください。</span><br></pre></td></tr></table></figure><h3 id="nn-Moduleとは？"><a href="#nn-Moduleとは？" class="headerlink" title="nn.Moduleとは？"></a>nn.Moduleとは？</h3><p>PyTorchでカスタムモデルを作成する際にクラスで継承するやつ</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">        self.act = nn.sigmoid()</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = self.fc1(x)</span><br><span class="line">        y = self.act(y)</span><br><span class="line">        y = self.fc2(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><span id="more"></span><h3 id="【事前知識】pythonの外部ファイルのクラスを実行可能にする方法"><a href="#【事前知識】pythonの外部ファイルのクラスを実行可能にする方法" class="headerlink" title="【事前知識】pythonの外部ファイルのクラスを実行可能にする方法"></a>【事前知識】pythonの外部ファイルのクラスを実行可能にする方法</h3><p>カレントディレクトリのhogeファイルのfugaクラスを読み込む場合</p><h5 id="main-py"><a href="#main-py" class="headerlink" title="main.py"></a><code>main.py</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .hoge <span class="keyword">import</span> fuga </span><br></pre></td></tr></table></figure><p>カレントディレクトリのhogeファイルの全てのクラスを読み込む場合</p><h5 id="main-py-1"><a href="#main-py-1" class="headerlink" title="main.py"></a><code>main.py</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .hoge <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><p>カレントディレクトリのhoge<strong>ディレクトリ</strong>のFuga1,Fuga2クラスを読み込む場合</p><h5 id="main-py-2"><a href="#main-py-2" class="headerlink" title="main.py"></a><code>main.py</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .hoge <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><h5 id="hoge-init-py"><a href="#hoge-init-py" class="headerlink" title="/hoge/__init__.py"></a><code>/hoge/__init__.py</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> *</span><br><span class="line">__all__=[<span class="string">&quot;Fuga&quot;</span>,<span class="string">&quot;Fuga2&quot;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="hoge-fuga-py"><a href="#hoge-fuga-py" class="headerlink" title="/hoge/fuga.py"></a><code>/hoge/fuga.py</code></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class Fuga</span><br><span class="line"></span><br><span class="line">class Fuga2</span><br></pre></td></tr></table></figure><h2 id="どこに継承元のModuleクラスが書いてある？"><a href="#どこに継承元のModuleクラスが書いてある？" class="headerlink" title="どこに継承元のModuleクラスが書いてある？"></a>どこに継承元のModuleクラスが書いてある？</h2><p>外部ファイルが存在?<br>→<code>torch/nn/Module.py</code>は存在しない</p><p>書き方的に、<code>torch/nn</code>ディレクトリに存在<br>Pythonは外部ディレクトリを読み込み時に<code>__init__.py</code>を先に読む。<br>nn.Module→<code>nn</code>ディレクトリの<code>__init__.py</code>を読む<br>👉<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/__init__.py">source</a></p><p>しかし、ここにも書いていない</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .modules <span class="keyword">import</span> *  <span class="comment"># noqa: F403</span></span><br></pre></td></tr></table></figure><p>つまり、<code>nn/module/</code>を全て読み込んでいる。<br>→<code>nn/module/__init__.py</code>を見る<br>👉<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/__init__.py">source</a></p><p>Moduleクラスを読み込んでいる部分を発見!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .module <span class="keyword">import</span> Module</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line"></span><br><span class="line">__all__ = [</span><br><span class="line">    <span class="string">&#x27;Module&#x27;</span>, <span class="string">&#x27;Identity&#x27;</span>, <span class="string">&#x27;Linear&#x27;</span>, <span class="string">&#x27;Conv1d&#x27;</span>, <span class="string">&#x27;Conv2d&#x27;</span>, <span class="string">&#x27;Conv3d&#x27;</span>, <span class="string">&#x27;ConvTranspose1d&#x27;</span>,</span><br></pre></td></tr></table></figure><p><code>nn/module/module.py</code>にModuleクラスが存在しているはず。</p><p>👉<a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py">nn&#x2F;module.py</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>:</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Base class for all neural network modules.   </span></span><br><span class="line"><span class="string">.</span></span><br><span class="line"><span class="string">.</span></span><br><span class="line"><span class="string">.</span></span><br></pre></td></tr></table></figure><h2 id="インスタンス生成時に実行されるもの"><a href="#インスタンス生成時に実行されるもの" class="headerlink" title="インスタンス生成時に実行されるもの"></a>インスタンス生成時に実行されるもの</h2><p>インスタンス生成例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model=SimpleModel()</span><br></pre></td></tr></table></figure><p>👉<a href="https://github.com/pytorch/pytorch/blob/e3e84830aade59722d819bc5fa01922239494790/torch/nn/modules/module.py#L425">source</a></p><h4 id="setattr"><a href="#setattr" class="headerlink" title="__setattr__"></a><code>__setattr__</code></h4><p><code>__setattr__</code>: インスタンスで代入操作実行時に呼び出されるコンストラクタ</p><p><code>(代入された変数,代入された値)</code><br>→ex.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">super</span>().__setattr__(<span class="string">&#x27;_parameters&#x27;</span>, OrderedDict())</span><br></pre></td></tr></table></figure><p>→<code>_parameters</code>に代入された時、辞書順にソートする。</p><h4 id="Callable"><a href="#Callable" class="headerlink" title="Callable"></a><code>Callable</code></h4><ul><li><p><code>Callable</code> 関数の型のようなもの。</p></li><li><p><code>関数名 Callable([引数型], 返り値型)</code><br> <code>...</code>→Ellipsisという、Noneだけど、bool(…)でTrueを返すオブジェクト。今は無いけど、そのうちあることを示唆するような感じ。<br>つまり、<code>forward: Callable[..., Any]</code>→…はforwardの引数の型、Anyがforwardの出力の型</p></li><li><p><code>_forward_unimplemented</code>はそのまま呼び出すと<code>NotImplementedError</code>を吐く関数。<br>→forward関数を、何か実装する雛形として宣言する。</p></li></ul><p>つまりこのセクションで言いたいのは・・・<br><strong>Moduleクラスの継承はインスタンス作成時にforwardを絶対に実装する必要がある！</strong></p><h2 id="インスタンス呼び出し時に実行されるもの"><a href="#インスタンス呼び出し時に実行されるもの" class="headerlink" title="インスタンス呼び出し時に実行されるもの"></a>インスタンス呼び出し時に実行されるもの</h2><p><code>def __call__</code>はどこに？</p><p>これも同様に、Callableで宣言はして、<code>_call_impl</code>で置き換えている。<br><code> __call__ : Callable[..., Any] = _call_impl</code></p><p><code>_call_impl</code>は何をしている？</p><p><code>*hook</code>系のものは、そのhookがTrueの時実行するもの。(ex.forward pass, backward passで__call__の処理を変える。)<br>→一旦<code>self.forward</code>が実行されていると考えていい。<br>nn.Module継承先から呼び出す場合、継承先の<code>self.forward</code>が実行される。</p><p>つまりこのセクションで言いたいのは・・・<br><strong>クラスや型の機能を使ってfoward&#x2F;backwardで処理を変更可能なモデル実行の雛形を作っている。</strong></p>]]></content>
    
    
    <summary type="html">&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;この資料はラボ内自主勉強会に向けて書いたものです。間違い等ありましたら教えてください。&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h3 id=&quot;nn-Moduleとは？&quot;&gt;&lt;a href=&quot;#nn-Moduleとは？&quot; class=&quot;headerlink&quot; title=&quot;nn.Moduleとは？&quot;&gt;&lt;/a&gt;nn.Moduleとは？&lt;/h3&gt;&lt;p&gt;PyTorchでカスタムモデルを作成する際にクラスで継承するやつ&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title class_&quot;&gt;SimpleModel&lt;/span&gt;(nn.Module):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title function_&quot;&gt;__init__&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;self&lt;/span&gt;):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;built_in&quot;&gt;super&lt;/span&gt;().__init__()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.fc1 = nn.Linear(&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;100&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.act = nn.sigmoid()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.fc2 = nn.Linear(&lt;span class=&quot;number&quot;&gt;100&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title function_&quot;&gt;forward&lt;/span&gt;(&lt;span class=&quot;params&quot;&gt;self, x&lt;/span&gt;):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        y = self.fc1(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        y = self.act(y)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        y = self.fc2(y)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; y&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
    <category term="ML" scheme="https://blog.gojtieji.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>HuggingFaceのdatasetsにPRを出したところ他の部分にもcommitできた話</title>
    <link href="https://blog.gojtieji.com/2022/08/12/OSScommit/"/>
    <id>https://blog.gojtieji.com/2022/08/12/OSScommit/</id>
    <published>2022-08-11T15:31:41.000Z</published>
    <updated>2022-08-11T15:59:31.144Z</updated>
    
    <content type="html"><![CDATA[<p>HuggingFaceの<a href="https://github.com/huggingface/datasets">datasetsレポジトリ</a>に小さなPRを出したところ、色々と話が進み、最初のPR部分以外にもcommitできたので、その話を書きます。</p><h2 id="最初のPR"><a href="#最初のPR" class="headerlink" title="最初のPR"></a>最初のPR</h2><p>大学院の研究で<code>opus_gnome</code>という対訳コーパスでダウンロードしようとしたところ、ドキュメントのサンプルにミスを見つけたため、<a href="https://github.com/huggingface/datasets/issues/4805">ISSUEを立ち上げました</a>。</p><p>簡単そうな内容だったため、<a href="https://github.com/huggingface/datasets/pull/4806">forkしてPRを出しました</a>。</p><p>しばらくすると、huggingfaceのMLエンジニアの方から<a href="https://github.com/huggingface/datasets/pull/4806#issuecomment-1209016702">コメントが来ました</a>。</p><span id="more"></span><blockquote><p>Anyway, @gojiteji thanks for your contribution and this fix.</p></blockquote><blockquote><p>Once you have modified the opus_gnome dataset card, our Continuous Integration test suite performs some tests on it that make some additional requirements: the errors that appear have nothing to do with your contribution, but with these additional quality requirements.</p></blockquote><p>どうやら僕の変更と関係がないところで、コードの品質を満たしていないところがあるようです。</p><h2 id="CIテスト"><a href="#CIテスト" class="headerlink" title="CIテスト"></a>CIテスト</h2><p>自分には何がダメなのかよくわからなかったので、聞いてみると、マージする前の<a href="https://github.com/huggingface/datasets/runs/7741104780?check_suite_focus=true">CIテストで失敗しており</a>、</p><blockquote><p>No first-level heading starting with <code>Dataset Card for</code> found in README. Skipping further validation for this README.</p></blockquote><p>READMEが<a href="https://github.com/huggingface/datasets/blob/main/templates/README.md">テンプレート</a>に沿っていないとのこと。</p><blockquote><pre><code>[&#39;ara&#39;, &#39;cat&#39;, &#39;foo&#39;, &#39;gr&#39;, &#39;nqo&#39;, &#39;tmp&#39;] are not registered tags for &#39;language&#39;, reference at https://github.com/huggingface/datasets/tree/main/src/datasets/utils/resources/languages.json</code></pre></blockquote><p>また、既存の<a href="https://github.com/huggingface/datasets/tree/main/src/datasets/utils/resources/languages.json">languages.json</a>というファイルを元に、利用可能な言語を指定していますが、それが元のファイルにはないものが含まれているらしい。</p><p><a href="https://github.com/huggingface/datasets/pull/4806#issuecomment-1209035359">諸々の会話とAlbertさんの多大なるサポート</a>を経てそれらを修正したところ、mainブランチに無事マージできました。</p><p><code>opus_gnome</code>のデータセットのリンクにアクセスすると、僕の書いた部分が見れます。<br><a href="https://huggingface.co/datasets/opus_gnome">https://huggingface.co/datasets/opus_gnome</a></p><p>自分がよく使っているOSSに貢献できたのはとても嬉しい限りです。</p><p>とても好意的に受け入れてくださっため、自信もつきました。今後も積極的にOSSにPRを出していこうと思います。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;HuggingFaceの&lt;a href=&quot;https://github.com/huggingface/datasets&quot;&gt;datasetsレポジトリ&lt;/a&gt;に小さなPRを出したところ、色々と話が進み、最初のPR部分以外にもcommitできたので、その話を書きます。&lt;/p&gt;
&lt;h2 id=&quot;最初のPR&quot;&gt;&lt;a href=&quot;#最初のPR&quot; class=&quot;headerlink&quot; title=&quot;最初のPR&quot;&gt;&lt;/a&gt;最初のPR&lt;/h2&gt;&lt;p&gt;大学院の研究で&lt;code&gt;opus_gnome&lt;/code&gt;という対訳コーパスでダウンロードしようとしたところ、ドキュメントのサンプルにミスを見つけたため、&lt;a href=&quot;https://github.com/huggingface/datasets/issues/4805&quot;&gt;ISSUEを立ち上げました&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;簡単そうな内容だったため、&lt;a href=&quot;https://github.com/huggingface/datasets/pull/4806&quot;&gt;forkしてPRを出しました&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;しばらくすると、huggingfaceのMLエンジニアの方から&lt;a href=&quot;https://github.com/huggingface/datasets/pull/4806#issuecomment-1209016702&quot;&gt;コメントが来ました&lt;/a&gt;。&lt;/p&gt;</summary>
    
    
    
    
    <category term="OSS" scheme="https://blog.gojtieji.com/tags/OSS/"/>
    
  </entry>
  
  <entry>
    <title>ISUCON12予選に参加しました</title>
    <link href="https://blog.gojtieji.com/2022/07/24/isucon-q-12/"/>
    <id>https://blog.gojtieji.com/2022/07/24/isucon-q-12/</id>
    <published>2022-07-24T06:00:00.000Z</published>
    <updated>2022-07-24T09:51:11.721Z</updated>
    
    <content type="html"><![CDATA[<p>お久しぶりです。いろいろあって、今年初めてのブログを書くことになりました。この記事は、7月23日(土)に開催された<a href="https://isucon.net/archives/56571716.html">ISUCON12</a>の参加記録です。チーム名にゃんべるぐで学生枠で参加しました。ISUCONは10以来の2回目の参加でした。(昨年は院試のため不参加)</p><p>結論から言うと、Scoreは最高3222、追試で3058で終了し、予選敗退となりました。(<a href="https://isucon.net/archives/56838276.html">https://isucon.net/archives/56838276.html</a>)</p><h2 id="メンバー集めと事前準備"><a href="#メンバー集めと事前準備" class="headerlink" title="メンバー集めと事前準備"></a>メンバー集めと事前準備</h2><p>5月下旬、チームを組んでくれるメンバーを募集したところ、大学時代に所属していたITサークルから2人協力してくれることになりました。<span id="more"></span>ayumin(<a href="https://twitter.com/shoumoji">@shoumoji</a>)と、あるみかん(<a href="https://twitter.com/AluminumMikan">@AluminumMikan</a>)です。この度はありがとうございました。</p><blockquote class="twitter-tweet"><p lang="ja" dir="ltr">ゆる募<br>ISUCONチーム組んでくれる方🙋</p>&mdash; Koki Tanaka (@gojiteji) <a href="https://twitter.com/gojiteji/status/1531596294870867968?ref_src=twsrc%5Etfw">May 31, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><p>その後、以下のように3回に分けてdiscordのボイスチャンネルで勉強会を行いました。</p><ul><li>2年前に参加したISUCON10の復習とレギュレーション輪読</li><li>ISUCON11の予選問題をAWS上でサーバーを立てて練習</li><li>強い人のISUCON11予選参加記を見て、解法をまねる</li></ul><p>メンバー全員、同じサークルだったため、コミュニケーションに支障がなかったのはとても良かったと思います。</p><h2 id="予選当日"><a href="#予選当日" class="headerlink" title="予選当日"></a>予選当日</h2><p>タイムライン形式でやったことを書いていきます。</p><h4 id="９時"><a href="#９時" class="headerlink" title="９時"></a>９時</h4><p>起床成功<br><img src="https://blog.gojiteji.com/images/isucon-q-12/isucon12_wakeup.png"></p><h4 id="１０時"><a href="#１０時" class="headerlink" title="１０時"></a>１０時</h4><p>競技開始、mysqlと別でdocker-composeを使ってgoが実行されている、またmysqlと別でsqliteが使用されていることを把握。<br>各ソフトウェアのバージョンを把握し、特にアップデートできそうなものはないことを把握。<br>ベンチを回すとScoreが2400ぐらい出る。</p><h4 id="１１時"><a href="#１１時" class="headerlink" title="１１時"></a>１１時</h4><p>テナントDBと管理DBが存在し、前者がsqlite、後者がmysqlに入っていることを確認。<br>ayuminが作成してくれたコードで、new Relicで重いエンドポイントがcompetition ranking id であるとわかる。<br>sqliteのログの見方はわからず。<br><code>/GET /api/player/player/:player_id</code>の処理(<code>playerHandler</code>)にN+1があると把握。<br><code>playersAddHandler</code>にbulk insert修正できそうな場所を発見。<br>DBの<code>player_score</code>テーブルの<code>player_id</code>,<code>competition_id</code>カラムに複合インデックスを貼る。Scoreが3000いかないぐらいに上昇。</p><h4 id="１２時"><a href="#１２時" class="headerlink" title="１２時"></a>１２時</h4><p><code>player_id</code>に関するエンドポイントが重いと気づき、<code>player_id</code>にもインデックスを貼ったが、スコアが落ちる。<br>一旦、index貼り職人になる。<br>sqliteとmysqlの統合ができるかなと検討しつつ、触れなかった。</p><h4 id="１３時"><a href="#１３時" class="headerlink" title="１３時"></a>１３時</h4><p>SQLのスキーマをBIGINT-&gt;INTに変更してみるも、反映せず。<br>そのほかSQLを見回る。<br>11時に発見したbulk insert修正に取り掛かり、<code>displayNames</code>を配列で作って<code>ExecContext</code>で配列のままINSERTできるように修正するも、ベンチでScoreが0になる。</p><h4 id="１４時"><a href="#１４時" class="headerlink" title="１４時"></a>１４時</h4><p><code>displayNames</code>に格納する<code>dispense()</code>関数でidを振っていたので、そのidの振り方に問題があるのではないかと思い、<code>dispense()</code>関数を見に行くも、単に<code>AUTO_INCREMENT</code>しているだけで、解決せず。</p><h4 id="１５時"><a href="#１５時" class="headerlink" title="１５時"></a>１５時</h4><p>bulk insert修正できそうな場所をもう一箇所発見。<code>playerScoreRows</code>を配列のままINSERTできるように変更。これは素直にベンチが通り、Scoreが3200ぐらいに上昇。</p><h4 id="１６時"><a href="#１６時" class="headerlink" title="１６時"></a>１６時</h4><p>もう一度<code>displayNames</code>のbulk insertの修正に戻るも、解決せず。<br>log出力周りを削除する。ベンチの変化はわからなかったが、エラーは起きなかったので、反映。<br>N+1の箇所をもう一度見るも解決せず。</p><h4 id="１７時"><a href="#１７時" class="headerlink" title="１７時"></a>１７時</h4><p>サーバー1台をmysql用、もう一台をsqliteとgo用に分ける。<br>メンバーの作った動くブランチを全てmasterにマージ。<br>dockerの再起動を有効にする。<br>再起動試験ができることを確認。</p><h4 id="１８時"><a href="#１８時" class="headerlink" title="１８時"></a>１８時</h4><p>競技終了</p><h2 id="所感"><a href="#所感" class="headerlink" title="所感"></a>所感</h2><p>運営の方もおっしゃっていた通り、sqliteをmysqlの二つが存在しているのが今までと大きく異なる点で、「改善できそうだけど、簡単にうまく動かない」が多く発生しました。</p><p>踏み台サーバーもよく理解していなかった2年前に比べて、コーディングの問題が解けるようになったのは、少し成長できたように思います。</p><p>自分のできなかったインフラの整備やsqliteのログ取得、インデックスのチューニングなどに取り組んでくれたayuminとあるみかんには感謝です。<br>イベント全体を通して、とても楽しめました。関係者の皆さんありがとうございました。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;お久しぶりです。いろいろあって、今年初めてのブログを書くことになりました。この記事は、7月23日(土)に開催された&lt;a href=&quot;https://isucon.net/archives/56571716.html&quot;&gt;ISUCON12&lt;/a&gt;の参加記録です。チーム名にゃんべるぐで学生枠で参加しました。ISUCONは10以来の2回目の参加でした。(昨年は院試のため不参加)&lt;/p&gt;
&lt;p&gt;結論から言うと、Scoreは最高3222、追試で3058で終了し、予選敗退となりました。(&lt;a href=&quot;https://isucon.net/archives/56838276.html&quot;&gt;https://isucon.net/archives/56838276.html&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&quot;メンバー集めと事前準備&quot;&gt;&lt;a href=&quot;#メンバー集めと事前準備&quot; class=&quot;headerlink&quot; title=&quot;メンバー集めと事前準備&quot;&gt;&lt;/a&gt;メンバー集めと事前準備&lt;/h2&gt;&lt;p&gt;5月下旬、チームを組んでくれるメンバーを募集したところ、大学時代に所属していたITサークルから2人協力してくれることになりました。</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Translation between Ainu Language(ain) and Japanese(jpn)</title>
    <link href="https://blog.gojtieji.com/2022/03/31/ja-ai-trans/"/>
    <id>https://blog.gojtieji.com/2022/03/31/ja-ai-trans/</id>
    <published>2022-03-30T15:00:00.000Z</published>
    <updated>2022-11-03T09:05:27.169Z</updated>
    
    <content type="html"><![CDATA[<p>I’ve researched about machine translation between Ainu Language and Japanese.</p><p>Four of them (mT5 and ByT5 with both  directions) are available at <a href="https://huggingface.co/Language-Media-Lab">https://huggingface.co/Language-Media-Lab</a> .</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;I’ve researched about machine translation between Ainu Language and Japanese.&lt;/p&gt;
&lt;p&gt;Four of them (mT5 and ByT5 with both  directions) ar</summary>
      
    
    
    
    
    <category term="ML" scheme="https://blog.gojtieji.com/tags/ML/"/>
    
    <category term="Resarch" scheme="https://blog.gojtieji.com/tags/Resarch/"/>
    
    <category term="AI" scheme="https://blog.gojtieji.com/tags/AI/"/>
    
    <category term="Machine Translation" scheme="https://blog.gojtieji.com/tags/Machine-Translation/"/>
    
  </entry>
  
  <entry>
    <title>AIは新入生になりきれるか?</title>
    <link href="https://blog.gojtieji.com/2021/12/01/7816825e-f7e4-0e24-6448-9ec513e5e027/"/>
    <id>https://blog.gojtieji.com/2021/12/01/7816825e-f7e4-0e24-6448-9ec513e5e027/</id>
    <published>2021-11-30T15:00:00.000Z</published>
    <updated>2022-04-22T22:49:23.181Z</updated>
    
    <content type="html"><![CDATA[<p><strong>※これは<a href="https://qiita.com/advent-calendar/2021/huit">2021年HUITアドベントカレンダー</a>1日目の記事です。</strong></p><p>こんにちは。１週間ぐらい前に思いつきでやってみたものなので、雑な部分がありますが優しい目で見てください。</p><center><img src="https://blog.gojiteji.com/images/7816825e-f7e4-0e24-6448-9ec513e5e027/0.jpeg"/ width=520></center><p>先週あたりから、HUITのdiscordに鳥羽舞( tobamai )を名乗る1年生がいたのをご存知でしょうか？実は、彼(彼女)は<strong>AIが生成した文章をtimesに投稿していました</strong>。(名前は”I am a bot”を逆順にしたものです。)また、tobamaiさんのアイコンは、最初に生成された自己紹介文をText2Imageの画像生成AI、<a href="https://huggingface.co/spaces/flax-community/dalle-mini">DALLE mini</a>に入力して生成した画像です。</p><span id="more"></span><h1 id="技術解説"><a href="#技術解説" class="headerlink" title="技術解説"></a>技術解説</h1><h2 id="GPT-Generative-Pre-trained-Transformer"><a href="#GPT-Generative-Pre-trained-Transformer" class="headerlink" title="GPT (Generative Pre-trained Transformer)"></a>GPT (Generative Pre-trained Transformer)</h2><p>今回文章生成に使用したのはGPT-J-6Bと呼ばれる、EleutherAIが作成したモデルです。文字入力・文字出力のモデルです。</p><div class="iframely-embed"><div class="iframely-responsive" style="height: 140px; padding-bottom: 0;"><a href="https://github.com/kingoflolz/mesh-transformer-jax" data-iframely-url="//cdn.iframe.ly/api/iframe?card=small&url=https%3A%2F%2Fgithub.com%2Fkingoflolz%2Fmesh-transformer-jax&key=3df23b9b3c026ee42c4d9f50408c69d1"></a></div></div><script async src="//cdn.iframe.ly/embed.js" charset="utf-8"></script><br><p>GPT-J-6Bは、OpenAIが作成した<a href="https://arxiv.org/abs/2005.14165">GPT-3</a>をベースとしています。GPT-3とは、昨年発表されたTransformerをベースとした、570GBの教師なしデータを1750億個のパラメータで学習した事前学習モデルです。知らない人でも、その派生技術である<a href="https://copilot.github.com/">GitHub Copilot</a>(システムの名称はOpenAI Codexです)を触ったことがある人は多いのではないでしょうか。</p><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Meet GitHub Copilot - your AI pair programmer. <a href="https://t.co/eWPueAXTFt">https://t.co/eWPueAXTFt</a> <a href="https://t.co/NPua5K2vFS">pic.twitter.com/NPua5K2vFS</a></p>&mdash; GitHub (@github) <a href="https://twitter.com/github/status/1409883156333879300?ref_src=twsrc%5Etfw">June 29, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><p>GPT-3がMicrosoftの独占ライセンスにあり、API経由でしか利用できないのに対し、GPT-J-6Bはオープンソースとして利用可能です。名前にある通り、60億(6 Billion)個程の巨大なパラメータからなる事前学習済みモデルです。Zero -Shotの評価でGPT-3の67億個パラメータ版と同じくらいの精度だそうです。</p><p>学習は、入力に対する次の単語を予測するタスクを行っています。従って、自分が生成した文章を再び入力にすれば無限に文章が生成できます。</p><p>さらに、<strong>マルチタスクにも対応可能</strong>で、</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Translate English to Franch:&#123;英文&#125;</span><br></pre></td></tr></table></figure><p>と入力すれば翻訳タスクを実行でき、</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TL;DR:&#123;文章&#125;</span><br></pre></td></tr></table></figure><p>と入力すれば要約タスク、</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pythonでコードを記述してください。 </span><br><span class="line">numpyをインポート:</span><br></pre></td></tr></table></figure><p>とすればプログラムの生成も可能です。デモサイトが公開されているのですぐに試せます。</p><div class="iframely-embed"><div class="iframely-responsive" style="height: 140px; padding-bottom: 0;"><a href="https://6b.eleuther.ai/" data-iframely-url="//cdn.iframe.ly/api/iframe?url=https%3A%2F%2F6b.eleuther.ai%2F&key=3df23b9b3c026ee42c4d9f50408c69d1"></a></div></div><script async src="//cdn.iframe.ly/embed.js" charset="utf-8"></script><br><p>あまりGPT-3の詳細まで解説すると長くなるので、興味を持った方はググったり<a href="https://arxiv.org/abs/2005.14165">論文</a>を読んでみてみてください。今回のアドベントカレンダーと似たような検証はGPT-3でも行われいて、<a href="https://gigazine.net/news/20201008-gpt-3-reddit/">redditでbotが１週間英語で会話し続けていた</a>ことが昨年話題となりました。</p><h2 id="【補足1】-GPT-2"><a href="#【補足1】-GPT-2" class="headerlink" title="【補足1】 GPT-2"></a>【補足1】 GPT-2</h2><p>GPT-3の前の世代である、GPT-2はパラメータが公開されています。GPT-3のパラメータが最大1750億個に対して、GPT-2は最大15億個のパラメータからなります(数字からしてGPT-3は異常ですね)。Microsoftの作成した、LINEの女子高生AI「りんな」と会話したことのある方もいると思います。りんなはGPT-2により動作しています。</p><div class="iframely-embed"><div class="iframely-responsive" style="height: 140px; padding-bottom: 0;"><a href="https://huggingface.co/rinna" data-iframely-url="//cdn.iframe.ly/api/iframe?card=small&url=https%3A%2F%2Fhuggingface.co%2Frinna&key=3df23b9b3c026ee42c4d9f50408c69d1"></a></div></div><script async src="//cdn.iframe.ly/embed.js" charset="utf-8"></script><br><h2 id="【補足2】-Transformerとは"><a href="#【補足2】-Transformerとは" class="headerlink" title="【補足2】 Transformerとは"></a>【補足2】 Transformerとは</h2><p>2017年にGoogleから発表された論文Attention Is All You Needで示された構造です。</p><div class="iframely-embed"><div class="iframely-responsive" style="height: 140px; padding-bottom: 0;"><a href="https://arxiv.org/abs/1706.03762v5" data-iframely-url="//cdn.iframe.ly/api/iframe?url=https%3A%2F%2Farxiv.org%2Fabs%2F1706.03762&key=3df23b9b3c026ee42c4d9f50408c69d1"></a></div></div><script async src="//cdn.iframe.ly/embed.js" charset="utf-8"></script><br><p>それまでの機械学習×NLP(Natural Language Processing、自然言語処理)では、RNNやGRUといった回帰構造を用いることが多かったですが、TransformerはAttention(注目)機構を基本とした構造をとっています。2021&#x2F;11&#x2F;27現在NLPタスクのSOTA(state of the art)は<a href="https://paperswithcode.com/area/natural-language-processing">Trasformerベースのモデルにより独占されています</a>。また、最近では<a href="https://github.com/google-research/vision_transformer">Vision Tranformer</a>などが画像処理分野でも良い結果を出すことで知られています。以下は論文中に示されたTransformerのEncoder(左側)-Decoder(右側)ブロックの図です。</p><center><img src="https://i.imgur.com/eCHbwzI.png" height=250/></center><p>一見ギョッとしますが、よくみるとFeedFowardやAdd&amp;Normなど簡単な構成です。特徴的なのは図中オレンジのAttention(Multi-Head AttentionはAttentionを並列に複数に並べたもの)ブロックで、具体的に<br><img src="https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cmathrm%7BAttention%7D%28Q%2C+K%2C+V%29%3D%5Cmathrm%7Bsoftmax%7D+%5Cleft%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D+%5Cright%29V%5C%3B%5C%3B%5C%3B%5C%3B%28d_k%3A+%5Cmathrm%7Binput%7D+%5C%3B+%5Cmathrm%7Bdimension%7D%29" alt=""><br>という処理をしています。Tranformer含め、機械学習におけるAttentionを理解したい方は、<a href="https://www.youtube.com/watch?v=g5DSLeJozdw">この動画</a>がわかりやすいです。</p><h2 id="【補足3】-BERTとの違い"><a href="#【補足3】-BERTとの違い" class="headerlink" title="【補足3】 BERTとの違い"></a>【補足3】 BERTとの違い</h2><p>機械学習×NLPではBERT(Bidirectional Encoder Representations from Transformers)なら聞いたことがある人が多いと思います。</p><div class="iframely-embed"><div class="iframely-responsive" style="height: 140px; padding-bottom: 0;"><a href="https://arxiv.org/abs/1810.04805v2" data-iframely-url="//cdn.iframe.ly/api/iframe?url=https%3A%2F%2Farxiv.org%2Fabs%2F1810.04805&key=3df23b9b3c026ee42c4d9f50408c69d1"></a></div></div><script async src="//cdn.iframe.ly/embed.js" charset="utf-8"></script><br>これはGPTと同じくTransformer構造を持ったモデルですが、あるタスクを解く際に、BERTであればPre Training(事前学習)とFine Tuning(特定タスクへの特化)と、２段階に分けて学習(=パラメータを更新)していたのに対し、GPTではPre Trainingの後はPrompt Tuningといって、タスクを入力するがパラメータは更新しない方法をとっています。特に、後者の手法は最近注目を集め出しています。<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Fine-tuning is dead. Prompts have closed the gap.<br><br>&quot;The Power of Scale for Parameter-Efficient Prompt Tuning&quot;<a href="https://t.co/g5kxMjXs9j">https://t.co/g5kxMjXs9j</a> <a href="https://t.co/pC5OiMuKIG">pic.twitter.com/pC5OiMuKIG</a></p>&mdash; Ethan Caballero (@ethancaballero) <a href="https://twitter.com/ethancaballero/status/1384548232076959745?ref_src=twsrc%5Etfw">April 20, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><p>また、BERTは補足2にあるEncoderのみで構成されているのに対し、GPT-3はDecoderのみで構成されています。</p><h1 id="実装"><a href="#実装" class="headerlink" title="実装"></a>実装</h1><p>今回はこのモデルをTPUで推論処理を実行し、いくつか制約を設けてdiscordアカウントから投稿することにしました。ハイパーパラメータは変更していません。</p><ul><li>実行したコード<div class="iframely-embed"><div class="iframely-responsive" style="height: 140px; padding-bottom: 0;"><a href="https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb" data-iframely-url="//cdn.iframe.ly/api/iframe?url=https%3A%2F%2Fcolab.research.google.com%2Fgithub%2Fkingoflolz%2Fmesh-transformer-jax%2Fblob%2Fmaster%2Fcolab_demo.ipynb&key=3df23b9b3c026ee42c4d9f50408c69d1"></a></div></div><script async src="//cdn.iframe.ly/embed.js" charset="utf-8"></script><br></li></ul><p>NLPの最新モデルはよくhuggingfaceという会社がhuggingfaceライブラリに実装してくれています。<a href="https://huggingface.co/EleutherAI/gpt-j-6B">GPT-6-JBも存在するため、</a>最初はそれでやろうと思ったのですが、CPU・GPU環境でやろうとすると、11GB程度のモデルをロードする際に毎回12GBあるRAMを食い尽くしてクラッシュするのでやめました。</p><p>また、公序良俗に反する・陰謀論に関する内容をたまに出力するため、timesへの投稿は文章を確認した上で手動で行なっています(discordにbotでないアカウントのメッセージ送信APIがなかったのも理由です)。検証の実行前に、HUITの運営の一人には確認をとっています。</p><h2 id="【補足4】-TPU"><a href="#【補足4】-TPU" class="headerlink" title="【補足4】 TPU"></a>【補足4】 TPU</h2><p>CPU&#x3D;Central Processing Unit、GPU&#x3D;Graphics Processing Unitのように、TPU&#x3D;Tensor Processing Unitの略です。Googleが開発したプロセッサで、最近発売されたPixel6にも搭載されています。</p><center><img src="https://i.imgur.com/hbC8dN3.jpg" width=400 /><br><a href="https://www.youtube.com/watch?v=KwcodBLXK70">Google Tensor - Pixel 6 Launch</a>  2:32より</center><p>機械学習では行列演算を大量に行うため、同様に行列演算を行うグラフィックス処理に用いられるGPUを使用して計算しがちです。TPUではさらに演算精度を落として高速化するなど、ニューラルネットの学習・推論に特化させた設計をしています。</p><p>詳しく知りたい方はGoogle Cloudの公式資料をご参照ください。</p><div class="iframely-embed"><div class="iframely-responsive" style="height: 140px; padding-bottom: 0;"><a href="https://cloud.google.com/blog/" data-iframely-url="//cdn.iframe.ly/api/iframe?card=small&url=https%3A%2F%2Fcloud.google.com%2Fblog%2Fja%2Fproducts%2Fgcp%2Fwhat-makes-tpus-fine-tuned-for-deep-learning&key=3df23b9b3c026ee42c4d9f50408c69d1"></a></div></div><script async src="//cdn.iframe.ly/embed.js" charset="utf-8"></script><br><h1 id="処理手順"><a href="#処理手順" class="headerlink" title="処理手順"></a>処理手順</h1><p>次のように全てPromptで実行しました。</p><ul><li>{設定文}+{いくつかの事前QA文章}を入力した上で、<code>人間:名前を含めて自己紹介をしてください</code>に対する回答を最初のtimesに投稿する</li><li>不定期に<code>人間:次のニュースの感想を教えてください</code>+{ニュース記事タイトル}を入力して感想を生成し、記事リンクと共にtimesへ投稿する。(<u><strong>つまり、記事の中身は読んでいません</strong></u>)</li><li>リプライは<code>人間:</code>のプレフィックスをつけて入力し、<code>AI:</code>以降の文章を改行が来るまでの区切りを応答とする。</li><li>生成した文章が怪しい時は再生成する。(<strong>注:故に全自動には至っていません</strong>)</li><li>全ての質問と回答は次の入力文章として追加する。</li><li>画像やスタンプに対しては返答しない。</li></ul><p>なお、最初の入力に使用した文字列は以下の通りです</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">以下はAIと人間の会話です。AIは19歳大学一年生という設定です。AIは優しく、テクノロジーに詳しいです。AIの名前は鳥羽舞です。</span><br><span class="line">答えられない質問には「今は答えられません」と回答します。</span><br><span class="line"></span><br><span class="line">人間:あなたはどんな人ですか?</span><br><span class="line">AI:鳥羽舞です。現在北海道大学に通っている総合理系の大学1年生です。</span><br><span class="line"></span><br><span class="line">人間:何コースを志望しますか</span><br><span class="line">AI:メディアネットワークコース志望です。機械学習を勉強したいです。</span><br><span class="line"></span><br><span class="line">人間:将来の夢はなんですか?</span><br><span class="line">AI:将来は機械学習のエンジニアもしくは研究者になりたいです。</span><br><span class="line"></span><br><span class="line">人間:どんなことに興味がありますか?</span><br><span class="line">AI:数学と機械学習に興味があります、機械学習は本でよく勉強しています。</span><br><span class="line"></span><br><span class="line">人間:好きな科目を教えてください</span><br><span class="line">AI:私の好きな科目は数学です。ベクトル解析と行列を解くのが好きです。</span><br><span class="line"></span><br><span class="line">人間:趣味はなんですか</span><br><span class="line">AI:趣味はプログラミングです。最近はPythonでdjangoを触っています。</span><br><span class="line"></span><br><span class="line">人間:好きなプログラミング言語はなんですか?</span><br><span class="line">AI:Pythonです。簡単に機械学習を触ったことがあります。</span><br><span class="line"></span><br><span class="line">人間:今日何をしましたか?</span><br><span class="line">AI:今日はゼロから学ぶディープラーニングを20ページぐらい読み進めました!</span><br><span class="line"></span><br><span class="line">人間:名前を含めて自己紹介をしてください</span><br><span class="line">AI:</span><br></pre></td></tr></table></figure><p>この続きとして生成された文章は次のとおりです。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">鳥羽舞です。現在は北海道大学の総合理系の大学1年生です。</span><br></pre></td></tr></table></figure><p>設定を保った文章が生成されました。</p><p>この文章の続きをどんどん生成していくというのが今回の流れです。</p><h1 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h1><p>初日にはZennの人気記事タイトルを引っ張ってきて次のように入力しました。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">人間:次のニュースの感想を教えてください「Next.js 12について本気出して和訳してみた」</span><br><span class="line">AI:</span><br></pre></td></tr></table></figure><p>生成結果は以下の通りです。見覚えがある人もいると思います。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Next.js 12について、一部のクロスポイント対応を修正しました。それでも不具合が残っています。</span><br></pre></td></tr></table></figure><p><a href="https://discord.com/channels/543343653394055169/913362457181978645/913393220388544562">その後、微妙に会話が生まれました。</a>会話に参加してくれた方はありがとうござます。クロスポイントって結局なんなんでしょうね?学習データのどんなところに出てきた言葉なのか気になります。</p><p>他にも、こんなパターンが生成できました。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">機械学習を勉強するとなると何もニュースに興味がなくなってしまいます。</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Next.jsは非常に機械学習派のフレームワークです。微妙に世代が違うことにより、ネイティブの使い方を学ぶことができます。HTMLビューを発見したことで、クリーンなWebによって高い評価を受けることができます。</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Next.js 12はすばらしい機能が入っています。私の上司には最近Next.jsを使った2つのWebアプリケーションを提案することが書いてありました。一瞬言葉に出ることなく、私たちは最高です。私は下のページを使って午後のプレゼンがしたいと思います。ぜひご覧ください。</span><br></pre></td></tr></table></figure><p>全てのチャット履歴を見たい方はHUTIのdiscordのtimes_tobamaiを覗いてください。(もっと会話してほしかったorz)</p><p>動かしてみると、英語での質疑応答に比べて、日本語は回答としての形式はほぼ保たれるが、内容がおかしな回答が多かったです。単純にデータセットに日本語が少ないという理由な気がします。**<a href="https://arxiv.org/abs/2101.00027">学習データのほとんどは英語</a>**なので、逆にここまで日本語でできたことの方がすごいかもしれません。</p><p>リアルタイムでtimesを見ていた方々は違和感を感じたでしょうか?</p><div data-paperform-id="ite64xwi"></div><script>(function() {var script = document.createElement('script'); script.src = "https://paperform.co/__embed.min.js"; document.body.appendChild(script); })()</script>    <h2 id="【補足5】-Human-in-the-Loop-HITL"><a href="#【補足5】-Human-in-the-Loop-HITL" class="headerlink" title="【補足5】 Human in the Loop (HITL)"></a>【補足5】 Human in the Loop (HITL)</h2><p>AIの出した結果に対して、人間の判断で出力の誤りを訂正し、再度学習させることをHuman in the Loopと言います。</p><h1 id="没コメント集"><a href="#没コメント集" class="headerlink" title="没コメント集"></a>没コメント集</h1><p>最後に、残念ながら再生成することにしたものの、興味深い生成結果だったものを残しておきます。</p><p>自己紹介を生成してみたら</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">人間:名前を含めて自己紹介をしてください</span><br><span class="line">AI:「人間は決して賢いではない。むしろ危険な動物の一種だ。だから、私は人間にはその天敵ともつながるものになりたいのだ」</span><br></pre></td></tr></table></figure><p>いきなり怖い。</p><p>性別を設定していなかったので聞いてみると…</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">人間:性別を教えてください</span><br><span class="line">AI:私は自身の分からない性別です。</span><br></pre></td></tr></table></figure><p>うーん、確かに??</p><p>何回もニュース記事の感想を聞いていると…</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">人間:次のニュースの感想を教えてください「Four Deep Learning Papers to Read in December 2021」</span><br><span class="line">AI:「次のニュース」のいいなりすぎて黙ってしまいました。</span><br></pre></td></tr></table></figure><p>申し訳ない</p><p><a href="https://speakerdeck.com/sorami/wap-nlp-tech-talk-number-4?slide=29">オープンソースに関するニュース</a>を聞いてみると</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">人間:次のニュースの感想を教えてください「オープンソースとしての形態素解析器Sudachi」</span><br><span class="line">AI:関連がないニュースの話です。オープンソースに賛同できないので、触れません。</span><br></pre></td></tr></table></figure><p><strong><h3>お前もオープンソースやろが!!</h3></strong></p><br>        <h1 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h1><p>GPT-2やGPT-3はその生成文章の精度の高さから、<a href="https://xtech.nikkei.com/atcl/nxt/column/18/00141/102100139/">フェイクニュースの生成・世論操作が可能な問題が指摘されています</a>。くれぐれも悪用することが無いようにしてください。</p><h3 id="修正"><a href="#修正" class="headerlink" title="修正"></a>修正</h3><ul><li><p>2021&#x2F;12&#x2F;01<br>  アンケートをチェックボックスからスケーラーに変更しました。</p></li><li><p>2021&#x2F;12&#x2F;02<br>  アンケートを締め切りました。<br>  質問「投稿期間中、tobamaiの発言に違和感はありましたか?」に対して</p><ul><li>なかった</li><li>少しあった</li><li>あった</li><li>結構あった</li></ul><p>  の4択で実施したところ、全て「結構あった」でした。(n&#x3D;3)<br>  サークルメンバーに聞いたところ、気持ちわるさがあった・怪しかったという感想がありました。どうしても英語データに引っ張られた部分があったように思います。日本語で事前学習したhyperCLOVAが気になります。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;※これは&lt;a href=&quot;https://qiita.com/advent-calendar/2021/huit&quot;&gt;2021年HUITアドベントカレンダー&lt;/a&gt;1日目の記事です。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;こんにちは。１週間ぐらい前に思いつきでやってみたものなので、雑な部分がありますが優しい目で見てください。&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://blog.gojiteji.com/images/7816825e-f7e4-0e24-6448-9ec513e5e027/0.jpeg&quot;/ width=520&gt;&lt;/center&gt;

&lt;p&gt;先週あたりから、HUITのdiscordに鳥羽舞( tobamai )を名乗る1年生がいたのをご存知でしょうか？実は、彼(彼女)は&lt;strong&gt;AIが生成した文章をtimesに投稿していました&lt;/strong&gt;。(名前は”I am a bot”を逆順にしたものです。)また、tobamaiさんのアイコンは、最初に生成された自己紹介文をText2Imageの画像生成AI、&lt;a href=&quot;https://huggingface.co/spaces/flax-community/dalle-mini&quot;&gt;DALLE mini&lt;/a&gt;に入力して生成した画像です。&lt;/p&gt;</summary>
    
    
    
    
    <category term="NLP" scheme="https://blog.gojtieji.com/tags/NLP/"/>
    
    <category term="GPT" scheme="https://blog.gojtieji.com/tags/GPT/"/>
    
  </entry>
  
  <entry>
    <title>JPHACKS2021 Award Dayに参加しました!</title>
    <link href="https://blog.gojtieji.com/2021/11/22/jphacks2021-ad/"/>
    <id>https://blog.gojtieji.com/2021/11/22/jphacks2021-ad/</id>
    <published>2021-11-21T16:11:13.000Z</published>
    <updated>2022-04-22T22:49:23.183Z</updated>
    
    <content type="html"><![CDATA[<p>※**<a href="https://blog.gojiteji.com/2021/11/01/jphacks2021/">以前のブログ</a>** の続きとなります。</p><p><img src="https://blog.gojiteji.com/images/jphacks2021-2/thumb.png"></p><p>11月20日にJPHacksの全国大会に相当する、JPHacks Award Dayに参加しました。ベースとなるアイデアや技術は以前のブログに書いたので、今回までの約２週間で行なった改善点について書いていきたいと思います。</p><span id="more"></span><h2 id="プロダクトの方向性の変更"><a href="#プロダクトの方向性の変更" class="headerlink" title="プロダクトの方向性の変更"></a>プロダクトの方向性の変更</h2><p>JPHacksの審査基準・審査方法は<a href="https://jphacks.github.io/2021-guideline/criteria/">最初から公開されており</a>、それに当てはまる方向で機能開発を進めていました。ところが、Hacking Sporint後にslackで”本選に向けたアドバイス”なる長文が回ってきました。要約すると「PSF(Problem Solution Fit)からPMF(Product Market Fit)への変更するといいよ」って内容です。<br>自分達もココノマスクは「正直ネタ枠」などと言っていたので、競合なんぞなく、<code>問題着眼点</code>と<code>完成度・動作性</code>の二点突破を目指していました。悩みに悩み、チームと何度もMTGを重ねた上で、<code>実行・実現可能性</code>の採点ポイントを上げるために「使いやすい帽子型にすること」「コストの観点から家族で共有するデバイスにすること」を決めました。</p><h2 id="機能開発"><a href="#機能開発" class="headerlink" title="機能開発"></a>機能開発</h2><p>僕が直接開発に関わった機能はオンデバイス顔認識の導入でした。導入理由は以下のような流れでした。</p><ol><li><p>コストの観点から家族で共有するデバイスにしたい</p></li><li><p>家族全員がメアドを持っているわけでは無いので、生体認証が現実的</p></li><li><p>家族の顔画像をクラウドで保存するのはプライバシー的に問題なので顔画像の処理はデバイス上で行う(<strong>COCOAがプライバシーに配慮した設計であるので、それを使う限りココノマスクもプライバシーに配慮した設計にする</strong>)</p></li></ol><p>顔認証はposeへの多様性がある<a href="https://arxiv.org/abs/1710.08092">vggface</a>を学習したInceptionnetのPytorchモデルをONNX形式に変換し、ONNXRuntimeで軽量化して実行する方針にしました。開発チームはweb技術に長けていたことから、ネイティブではなくwebアプリとして作っていたため、最初はJavaScript版のONNXRuntimeで推論処理を実行しようとしましたが、</p><ul><li>ONNX Runtime Webの提供開始が最近であり、サンプルやエラー(主にWebAssembly周り)の情報が少ない</li><li>webである限り、最初の接続時にモデルをロードする時間がかかる</li></ul><p>の二つの問題が発生しました。そこで妥協点ではありますが、ラズパイ上でPython版ONNX Runtimeを実行するようにしました(ラズパイとスマホはbluetooth接続)。</p><h2 id="チーム開発で気をつけたこと"><a href="#チーム開発で気をつけたこと" class="headerlink" title="チーム開発で気をつけたこと"></a>チーム開発で気をつけたこと</h2><p>僕はReactがそこまで書けないのですが、PRのCode Reviewやcommentを積極的にするように心がけました。逆に自分のコードに対しても書いてない人からは分かりにくいだろうと思い、<a href="https://reviewnb.com/">ReviewNB</a>の導入や、MLで使用したモデルやデータセットの発表論文を説明するようにしました。<br>自分がB1,B2相当の頃にはブランチ切っていたかどうかも怪しかったので、B1,B2のチームメイトの方々にはGitHubの機能を使い倒してすごいなぁと感心していました。</p><h2 id="その他"><a href="#その他" class="headerlink" title="その他"></a>その他</h2><ul><li><p>資料作成</p><p> 発表スライドにCanvaを使いました。アニメーションの追加や画像からの色抽出機能が便利で、プレゼンというよりも動画編集に近い感覚で作れました。</p> <div style="position: relative; width: 100%; height: 0; padding-top: 56.2500%; padding-bottom: 48px; box-shadow: 0 2px 8px 0 rgba(63,69,81,0.16); margin-top: 1.6em; margin-bottom: 0.9em; overflow: hidden; border-radius: 8px; will-change: transform;">  <iframe loading="lazy" style="position: absolute; width: 100%; height: 100%; top: 0; left: 0; border: none; padding: 0;margin: 0;"    src="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAEv-7H1qU4&#x2F;view?embed">  </iframe></div>Koki Tanakaさんによる<a href="https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAEv-7H1qU4&#x2F;view?utm_content=DAEv-7H1qU4&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link" target="_blank" rel="noopener">A_2111 チームガリガリ君</a></li><li><p>開発環境が地獄</p><p>  今回の開発での一番のボトルネックはWebアプリにしたことだと思います。僕はMac&#x2F;iOSの環境しかないので、WebBluetoothは使えない・Windows版Chromeと同じ機能がMacOS版Chromeでは再現されないなどが発生したので、デバイスとアプリの接続テストが後回しになっていました。また、機械学習に関してもネイティブアプリで実行可能なAPIの方がパフォーマンスが良い場合が多く、今までのハッカソンでも「Webよりネイティブの方が映えるよな〜」と思うことは多々ありましたが、ここまで開発面からwebであることを悔やむ経験は初めててでした。</p></li></ul><h2 id="全体を通しての感想"><a href="#全体を通しての感想" class="headerlink" title="全体を通しての感想"></a>全体を通しての感想</h2><p>今回のハッカソンで個人的に嬉しかったことが2つありました。<br>1つ目はハッカソンにML技術を自然な形で利用できたこと。今まで自分は開発(web)と研究(ML)は完全に切り離された状態で勉強してきて、その二つが混じり合う経験をしてこなかったのがコンプレックスに感じていました(過去にもブログでそんなことを書いた気がします。)。ですが、今回プライバシーやUX向上を理由に、推論処理だけではありますが開発に応用できたのは楽しかったです。<br>2つ目はJPhackのgithubの<a href="https://github.com/jphacks">Popular repositories</a><br>に載ったことです。これは自分達のチームメンバーに加えて、ツイートしたことでstarをしてくれた方々による影響です。ハッカソン前に過去の優秀な作品のコードをよくPopular repositoriesから参照していたので、ここに載れたのは嬉しい限りです。<br><img src="https://blog.gojiteji.com/images/jphacks2021-2/SS.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;※**&lt;a href=&quot;https://blog.gojiteji.com/2021/11/01/jphacks2021/&quot;&gt;以前のブログ&lt;/a&gt;** の続きとなります。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.gojiteji.com/images/jphacks2021-2/thumb.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;11月20日にJPHacksの全国大会に相当する、JPHacks Award Dayに参加しました。ベースとなるアイデアや技術は以前のブログに書いたので、今回までの約２週間で行なった改善点について書いていきたいと思います。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Web" scheme="https://blog.gojtieji.com/tags/Web/"/>
    
    <category term="ML" scheme="https://blog.gojtieji.com/tags/ML/"/>
    
    <category term="hackathon" scheme="https://blog.gojtieji.com/tags/hackathon/"/>
    
    <category term="IoT" scheme="https://blog.gojtieji.com/tags/IoT/"/>
    
    <category term="Bluetooth" scheme="https://blog.gojtieji.com/tags/Bluetooth/"/>
    
  </entry>
  
  <entry>
    <title>JPHACKS2021に参加しました!</title>
    <link href="https://blog.gojtieji.com/2021/11/01/jphacks2021/"/>
    <id>https://blog.gojtieji.com/2021/11/01/jphacks2021/</id>
    <published>2021-11-01T10:54:56.000Z</published>
    <updated>2022-04-22T22:49:23.183Z</updated>
    
    <content type="html"><![CDATA[<p>お久しぶりです。ブログを書くのも約半年ぶりとなりました。タイトルにある通り、<a href="https://jphacks.com/">JPHACKS2021</a> Hacking Sprint(いわゆる地方予選に相当) に参加してきました。</p><p>結果から申しますと、なんと<strong>Aブロック(北海道・東北ブロック)で優勝しました！！🏅🎉</strong></p><h2 id="作ったもの"><a href="#作ったもの" class="headerlink" title="作ったもの"></a>作ったもの</h2><p>今回、サークル<a href="https://huitgroup.github.io/">HUIT</a>のメンバー私含め5人と共に出場しました。私たちが作ったのは、COCOA(新型コロナウイルス接触確認アプリ)の信号でマスクを自動開閉するアプリとデバイス、サムネイルにもある「ココノマスク」を作成しました。</p><span id="more"></span><p><img src="https://pbs.twimg.com/media/FC9ayn-aUAAaavJ?format=jpg&name=large" alt="ココノマスクのアプリとデバイス"></p><p>私は主にデバイスの作成を行いました。使用技術は以下の通りです。<br><img src="https://pbs.twimg.com/media/FC9bKszacAEb6PR?format=jpg&name=large" alt="使用した技術"></p><p>デバイスはRaspberry Pi Zeroの上にPythonのプログラムで動作しており、これがBluetooth LEを見張ることで、COCOAの信号を識別します。チームにはネイティブアプリをかける人がいなかったため、Web(React.js)をベースにアプリを開発しました。従って、Web Bluetoothをデバイスとのペアリングに採用しました。</p><p>デバイスの動作こんな感じです(GIF)。</p><blockquote class="twitter-tweet"><p lang="und" dir="ltr"><a href="https://t.co/onBxV0Fxxw">pic.twitter.com/onBxV0Fxxw</a></p>&mdash; Koki (@gojiteji) <a href="https://twitter.com/gojiteji/status/1454487043610796035?ref_src=twsrc%5Etfw">October 30, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><p>作っていて面白かったものは、COCOAの通信の仕様書<a href="https://www.google.com/covid19/exposurenotifications/">Google&#x2F;Apple Exposure Notifications System</a>を読んでいた時です。一年ほど前になんとなくインストールしていたCOCOAの裏側ではいくつものプライバシーを配慮した技術があり(ex.送信電力の暗号化)、これには逆にCOCOAを応用した設計をするのにに苦労する場面もありました。実装の詳細は<a href="https://github.com/jphacks/A_2111">GitHubを参照ください</a>。実は私は数年前にBLE通信で心が折れた経験があり、正直完成するかめちゃめちゃ不安でした。</p><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>本ブログに何度か登場している<a href="https://twitter.com/takapiro_99">たかぴろ</a>とはおそらく最後になるハッカソンイベントだったので、共に栄誉ある賞を受賞できたことがとても嬉しかったです。また、他3人のメンバーについても、僕と初対面にも関わらずチーム開発に受け入れてくれた事にも感謝します。<br>1週間でハードウェア・ソフトウェア共に突貫工事で進めたため、Award Day 進出になった際にはもっと技術を磨こうと思います。</p><p>最後に、発表で使用したスライドを載せておきます。</p><iframe src="https://docs.google.com/presentation/d/e/2PACX-1vSDdz9wtoaqWwp9J3if5AZR-36ecevJdxNyXyJ7Ji4ghFfAptug0QxjFISP8JmJCWudaysbajGu4hpY/embed?start=false&loop=false&delayms=3000" frameborder="0" width="320" height="190" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe><h2 id="links"><a href="#links" class="headerlink" title="links"></a>links</h2><ul><li><p>GitHub<br>  <a href="https://github.com/jphacks/A_2111">https://github.com/jphacks/A_2111</a></p></li><li><p>Exposure Notifications System<br>  <a href="https://www.google.com/covid19/exposurenotifications">https://www.google.com/covid19/exposurenotifications</a></p></li><li><p>たかぴろと参加したハッカソン記事</p><ul><li>I created an AR haiku app.<br>  <a href="https://blog.gojiteji.com/2021/02/19/unitygps/">https://blog.gojiteji.com/2021/02/19/unitygps/</a></li><li>JPhacks 2020　award dayに進出&amp;参加しました!<br>  <a href="https://blog.gojiteji.com/2020/11/28/jphacks2020/">https://blog.gojiteji.com/2020/11/28/jphacks2020/</a></li><li>Discovery hackathon 2020 参加記<br>  <a href="https://blog.gojiteji.com/2020/10/26/discoveryhackathon/">https://blog.gojiteji.com/2020/10/26/discoveryhackathon/</a></li><li>北大ハッカソン参加記<br>  <a href="https://blog.gojiteji.com/2020/08/03/hu-hack/">https://blog.gojiteji.com/2020/08/03/hu-hack/</a></li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;お久しぶりです。ブログを書くのも約半年ぶりとなりました。タイトルにある通り、&lt;a href=&quot;https://jphacks.com/&quot;&gt;JPHACKS2021&lt;/a&gt; Hacking Sprint(いわゆる地方予選に相当) に参加してきました。&lt;/p&gt;
&lt;p&gt;結果から申しますと、なんと&lt;strong&gt;Aブロック(北海道・東北ブロック)で優勝しました！！🏅🎉&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;作ったもの&quot;&gt;&lt;a href=&quot;#作ったもの&quot; class=&quot;headerlink&quot; title=&quot;作ったもの&quot;&gt;&lt;/a&gt;作ったもの&lt;/h2&gt;&lt;p&gt;今回、サークル&lt;a href=&quot;https://huitgroup.github.io/&quot;&gt;HUIT&lt;/a&gt;のメンバー私含め5人と共に出場しました。私たちが作ったのは、COCOA(新型コロナウイルス接触確認アプリ)の信号でマスクを自動開閉するアプリとデバイス、サムネイルにもある「ココノマスク」を作成しました。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Web" scheme="https://blog.gojtieji.com/tags/Web/"/>
    
    <category term="hackathon" scheme="https://blog.gojtieji.com/tags/hackathon/"/>
    
    <category term="IoT" scheme="https://blog.gojtieji.com/tags/IoT/"/>
    
    <category term="Bluetooth" scheme="https://blog.gojtieji.com/tags/Bluetooth/"/>
    
  </entry>
  
  <entry>
    <title>Heroku(Python・Nodejs)開発中の備忘録</title>
    <link href="https://blog.gojtieji.com/2021/05/09/heroku/"/>
    <id>https://blog.gojtieji.com/2021/05/09/heroku/</id>
    <published>2021-05-08T17:05:24.000Z</published>
    <updated>2022-04-22T22:49:23.182Z</updated>
    
    <content type="html"><![CDATA[<p>先日、herokuのpythonアドオンで開発中に、内部でnodejsを実行したい場面に出くわした。shellに入って環境構築しようと、<code>heroku run bash</code>でdynoの内部に入ったが、<code>apt get install node</code>が正常に動作しなかった。</p><span id="more"></span><p>結果から言うと、addonでPythonと同じくnode.jsを追加すればよい。<br>なお、同様に<code>apt install ffmpeg</code>ができなくなったが、herokuにはbuildpackと言うものがあり、一部登録されているものは<br><code>heroku buildpacks:add https://github.com/jonathanong/heroku-buildpack-ffmpeg-latest.git</code><br>でインストールが可能らしい。</p><p><img src="https://blog.gojiteji.com/images/heroku/01.png" alt="一通りインストールした後のherokuのコンソール画面"></p><p>-参考<br>    - herokuで正式にサポートされているbuildpack<br>        <a href="https://devcenter.heroku.com/ja/articles/buildpacks">https://devcenter.heroku.com/ja/articles/buildpacks</a><br>    - Herokuでffmpegを使えるようにする<br>    <a href="https://qiita.com/akabei/items/8fd0dc0c2cd2c9cad0c0">https://qiita.com/akabei/items/8fd0dc0c2cd2c9cad0c0</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;先日、herokuのpythonアドオンで開発中に、内部でnodejsを実行したい場面に出くわした。shellに入って環境構築しようと、&lt;code&gt;heroku run bash&lt;/code&gt;でdynoの内部に入ったが、&lt;code&gt;apt get install node&lt;/code&gt;が正常に動作しなかった。&lt;/p&gt;</summary>
    
    
    
    
    <category term="備忘録" scheme="https://blog.gojtieji.com/tags/%E5%82%99%E5%BF%98%E9%8C%B2/"/>
    
    <category term="heroku" scheme="https://blog.gojtieji.com/tags/heroku/"/>
    
  </entry>
  
  <entry>
    <title>ABEMA Growth Techに参加しました。</title>
    <link href="https://blog.gojtieji.com/2021/03/14/growthtech/"/>
    <id>https://blog.gojtieji.com/2021/03/14/growthtech/</id>
    <published>2021-03-14T12:36:45.000Z</published>
    <updated>2022-04-22T22:49:23.182Z</updated>
    
    <content type="html"><![CDATA[<p>タイトルの通りABEMA Growth Techに参加しました。実施日は2021&#x2F;03&#x2F;14-15の二日間、リモートで行われました。内容としては一日目にサーバー構築とモデルの学習をメンターのサポートのもとに作り、二日目に自由課題をやるというものでした。</p><span id="more"></span><p>一日目、サーバー構築はgunicornを用いたpythonサーバーを作り、インスタンスをEC2に乗せるという構成でした。機械学習はABEMAの推薦を視聴データからLightGBMを用いて推測するというものでした。</p><p>二日目の自由課題は主にインフラ周りの改善を行いました。具体的にはAPIサーバーのコンテナ化とCIの自動化をCircleCIを用いて行いました。CircleCIはpush&#x2F;pullの自動化までは行えましたが、push後にDockerfileの自動ビルドを実装している間に終了時間となってしまいました。二日間であまり時間がなく。学習モデルの改善をさわりたかったのですが、手がつけられませんでした。</p><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>バックエンドと機械学習をここに勉強することがあっても、両方の視点から改善・結合する経験があまりなかったため、実装していてとても楽しかったです。また、自分が使っているサービスのデータで新たに機能を開発するのもとても興味深かったです。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;タイトルの通りABEMA Growth Techに参加しました。実施日は2021&amp;#x2F;03&amp;#x2F;14-15の二日間、リモートで行われました。内容としては一日目にサーバー構築とモデルの学習をメンターのサポートのもとに作り、二日目に自由課題をやるというものでした。&lt;/p&gt;</summary>
    
    
    
    
    <category term="internship" scheme="https://blog.gojtieji.com/tags/internship/"/>
    
    <category term="ML" scheme="https://blog.gojtieji.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>I created an AR haiku app.</title>
    <link href="https://blog.gojtieji.com/2021/02/19/unitygps/"/>
    <id>https://blog.gojtieji.com/2021/02/19/unitygps/</id>
    <published>2021-02-19T11:22:55.000Z</published>
    <updated>2022-04-22T22:49:23.184Z</updated>
    
    <content type="html"><![CDATA[<h2 id="「ここで一句（koko-de-ikku）」"><a href="#「ここで一句（koko-de-ikku）」" class="headerlink" title="「ここで一句（koko de ikku）」"></a>「ここで一句（koko de ikku）」</h2><p>For a joint project between a certain company and a university, I created a location-based AR haiku application, as the title of this article says. The name of the app is “ここで一句(Koko de Ikku)” . “ここ(koko)” has a double-meaning of the Japanese word for place and the word for the current time.</p><p><img src="https://blog.gojiteji.com/images/haiku/appicon.jpg"></p><iframe width="560" height="315" src="https://www.youtube.com/embed/SiJFCLP9V0o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><span id="more"></span><h2 id="App-Detail"><a href="#App-Detail" class="headerlink" title="App Detail"></a>App Detail</h2><p>This app consists of a 詠む(write) and 観る(see) screen.</p><ul><li><p>詠む(write)<br>  You can write haiku with your brush. When you write the top haiku(上の句), middle haiku（中の句), and bottom haiku(下の句), the haiku(句) will be saved with the location where you write it.</p></li><li><p>観る(see)<br>  As shown in the image below, when you press 観る(see), a list of haiku close to yor location will be displayed in AR.You can relive the scene where the haiku was actually written.</p></li></ul><p><img src="https://blog.gojiteji.com/images/haiku/displays.png"></p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>I developed an AR application with Unity, which is mainly the front-end side. I used AR Foundation as the framework.</p><p><img src="https://blog.gojiteji.com/images/haiku/architecture.png"></p><h2 id="Final-Thougts"><a href="#Final-Thougts" class="headerlink" title="Final Thougts"></a>Final Thougts</h2><p>It was my first time to develop an AR application.I tried to develop with flutter and swift, but finally decided to use Unity. Flutter was interesting to use, and I will try to make something with it in the future if I have time.</p><h2 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h2><ul><li><a href="https://github.com/gojiteji/unityAR">https://github.com/gojiteji/unityAR</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;「ここで一句（koko-de-ikku）」&quot;&gt;&lt;a href=&quot;#「ここで一句（koko-de-ikku）」&quot; class=&quot;headerlink&quot; title=&quot;「ここで一句（koko de ikku）」&quot;&gt;&lt;/a&gt;「ここで一句（koko de ikku）」&lt;/h2&gt;&lt;p&gt;For a joint project between a certain company and a university, I created a location-based AR haiku application, as the title of this article says. The name of the app is “ここで一句(Koko de Ikku)” . “ここ(koko)” has a double-meaning of the Japanese word for place and the word for the current time.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.gojiteji.com/images/haiku/appicon.jpg&quot;&gt;&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/SiJFCLP9V0o&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;</summary>
    
    
    
    
    <category term="Unity" scheme="https://blog.gojtieji.com/tags/Unity/"/>
    
    <category term="GPS" scheme="https://blog.gojtieji.com/tags/GPS/"/>
    
    <category term="AR" scheme="https://blog.gojtieji.com/tags/AR/"/>
    
  </entry>
  
  <entry>
    <title>Looking Back on 2020</title>
    <link href="https://blog.gojtieji.com/2020/12/31/2020/"/>
    <id>https://blog.gojtieji.com/2020/12/31/2020/</id>
    <published>2020-12-30T16:57:49.000Z</published>
    <updated>2022-04-22T22:49:23.181Z</updated>
    
    <content type="html"><![CDATA[<p>There is only one day left in this year. As the title suggests, I would like to look back on 2020 briefly.</p><span id="more"></span><h2 id="Early-2020"><a href="#Early-2020" class="headerlink" title="Early 2020"></a>Early 2020</h2><p>I started writing this blog. At that time, I was doing my graduation research on text-to-speech with emotions at a technical college, and I was struggling to get good results. In an attempt to output some of the results, I started posting articles on solving kaggle using the machine learning knowledge which I had learned in a process the research. Although I felt a sense of accomplishment when I posted articles, I was so stuck in my graduation research that I didn’t go to school much anymore. I drank alcohol every day to lift my depressed mood. Looking back now, these days were the worst for me in many ways.</p><h2 id="Mid-2020"><a href="#Mid-2020" class="headerlink" title="Mid 2020"></a>Mid 2020</h2><p>I entered university, but mind hadn’t cleared up at all since my graduation project. I wasn’t very motivated to go to school, but all my classes were online, so I was able to attend. I didn’t have any friends on campus, so I spent most of my time talking with a transfered student and my friends from the technical college. Fortunately, I he was a good person. The internship also fluctuated a lot, being cancelled and going online. I participated in my first online internship. I thought, “I can do it online!” At the time, but now I think  it’s better to do it offline.<br>The first time I made on-campus friends was at a hackathon in July. With this opportunity, I joined an IT circle and gradually made more friends. I participated in isucon10 with the people I met in the sago circle. (The result was not good.) After the hackathon, I felt like my mood was improving, but I started to feel depressed again, as if my mood is cyclical. It was around this time that I began to visit a psychosomatic department.</p><h2 id="Late-2020"><a href="#Late-2020" class="headerlink" title="Late 2020"></a>Late 2020</h2><p>I participated in the Discovery Hackathon and JPhacks. Although the results of both events were not so good,but it was great to be able to create something with my friends. In my personal development, I tend to do master push, but after this experience, I have started to make issues and create branches even in my personal development (I should have done this in the past). Some offline classes started, I got part-time technical jobs, and I was able to interact with more people than I could have imagined in April this year. I was able to join an NLP-related lab as I had hoped, and I was able to deepen my relationships with the senior membors.</p><p>In the past year, I have been able to post 18 articles every month, which is an excellent for me.I also had a lot of opportunities to use React and Go, which helped me grow technically. My mood has been improving lately, and I would like to continue to do my best in many ways at this rate.</p><p>In 2021, I want to try to write a blog with more content.</p><p>Have a great rest of the year.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;There is only one day left in this year. As the title suggests, I would like to look back on 2020 briefly.&lt;/p&gt;</summary>
    
    
    
    
    <category term="poem" scheme="https://blog.gojtieji.com/tags/poem/"/>
    
  </entry>
  
  <entry>
    <title>HUITに半年ぐらいいた感想</title>
    <link href="https://blog.gojtieji.com/2020/12/11/huit2020/"/>
    <id>https://blog.gojtieji.com/2020/12/11/huit2020/</id>
    <published>2020-12-11T10:53:21.000Z</published>
    <updated>2022-04-22T22:49:23.182Z</updated>
    
    <content type="html"><![CDATA[<p><strong>この記事は<a href="https://qiita.com/advent-calendar/2020/huit">HUITアドベントカレンダー2020 12日目</a>の記事です。</strong></p><h2 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h2><p>せっかくなので技術系の記事を書こうと思い、教師なし翻訳で遊ぶ記事を投稿しようとしていましたが、12日までに思うような結果が出なかったのでポエム記事に変更しました。いい感じにまとまったら当ブログに書こうと思います。暖かい目で見守ってください。（よかったら読んでね）</p><span id="more"></span><h2 id="HUIT入る前"><a href="#HUIT入る前" class="headerlink" title="HUIT入る前"></a>HUIT入る前</h2><p>今年鈴鹿高専から北海道大学3年に編入しました。編入生の集いに遅刻してしまい、編入生の友達ですら作ることに失敗してしまいました。コロナのせいで入学式もなく、同コースの編入生1人しか友達がいない状態でした。高専時代の技術サークル仲間とずっと会話していました。</p><h2 id="HUITに入った"><a href="#HUITに入った" class="headerlink" title="HUITに入った"></a>HUITに入った</h2><p>やっと友達ができたのは7月になってからです。<a href="https://blog.gojiteji.com/2020/08/03/hu-hack/">学内ハッカソン</a>の案内があり、それで一緒にチームになった<a href="https://twitter.com/takapiro_99">たかぴろ</a>さんにHUITのslackに入れてもらいました。</p><h2 id="HUITに入っていてできたこと"><a href="#HUITに入っていてできたこと" class="headerlink" title="HUITに入っていてできたこと"></a>HUITに入っていてできたこと</h2><ul><li><p>isucon予選に参加できた<br>  バックエンド書いてる学生の友達がいなかったので（高専の友達みんな就職しちゃった..）、今年のisuconは見送っていましたが、運よくHUIT内で追加メンバーの募集をしていて参加できました。とてもつよい方々で大変勉強になりました。</p></li><li><p>チームを組んでハッカソンに参加できた<br>  技術友達を増やせました。みんな強いです。</p></li><li><p>LT会に参加できた</p></li><li><p>AI関連のコミュニティに参加できた<br>  最近NLP関連技術を勉強しているのですが、twitter以外でコミュニティがなかったためこのコミュニティがあるのはありがたいです。</p></li><li><p>技術バイトをもらうことができた<br>  HUITの友達の友達の…っていう繋がりでベンチャーのバイトをもらいました。好きで物作りをすることはありましたが、実際のサービスリリース経験は少ないため、とても勉強になっています。</p></li></ul><h2 id="最後に"><a href="#最後に" class="headerlink" title="最後に"></a>最後に</h2><p>4~6月はオンラインでの人の繋がりしかなく、コミュニケーション不足で鬱な状態でした。オフラインでも繋がれるコミュニティがあるのは私自身のモチベーションアップやにもつながっています。もしまだHUITに入ってないよって人は是非入りましょう！</p><p>拙い文章になってしまいましたが、以上です。まだしゃべったことない人は仲良くしてください🙇‍♂️バックエンドやNLPに興味があります！<br>twitter:<a href="https://twitter.com/gojiteji">gojiteji</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;この記事は&lt;a href=&quot;https://qiita.com/advent-calendar/2020/huit&quot;&gt;HUITアドベントカレンダー2020 12日目&lt;/a&gt;の記事です。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h2&gt;&lt;p&gt;せっかくなので技術系の記事を書こうと思い、教師なし翻訳で遊ぶ記事を投稿しようとしていましたが、12日までに思うような結果が出なかったのでポエム記事に変更しました。いい感じにまとまったら当ブログに書こうと思います。暖かい目で見守ってください。（よかったら読んでね）&lt;/p&gt;</summary>
    
    
    
    
    <category term="poem" scheme="https://blog.gojtieji.com/tags/poem/"/>
    
  </entry>
  
  <entry>
    <title>JPhacks 2020　award dayに進出&amp;参加しました!</title>
    <link href="https://blog.gojtieji.com/2020/11/28/jphacks2020/"/>
    <id>https://blog.gojtieji.com/2020/11/28/jphacks2020/</id>
    <published>2020-11-28T13:29:08.000Z</published>
    <updated>2022-04-22T22:49:23.183Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://blog.gojiteji.com/images/jphacks2020/title.png"><br>お久しぶりです。<br>11&#x2F;7にJPHACKS予選、その後11&#x2F;28にJPHACKS本選に出場しました。その記録を書いておきます。(JPHACKS:<a href="https://jphacks.com/">https://jphacks.com/</a>)</p><span id="more"></span><h1 id="作ったもの"><a href="#作ったもの" class="headerlink" title="作ったもの"></a>作ったもの</h1><p>手のひら大のデバイスarcana(アルカナ)と、その連携webアプリを作成しました。arcanaは中に入っている重さセンサーで、重量を通して内容量をスマホから確認することができます。スマホから、冷蔵庫の中身が「あるかな」というのを確認できるデバイスとなっています。</p><p><img src="https://blog.gojiteji.com/images/jphacks2020/img4.png"></p><p>各々の容量はこのように確認できます。<br><img src="https://blog.gojiteji.com/images/jphacks2020/img3.png"></p><p>その他にも手の消毒アルコールを店が一括管理やその他液体、容量が少なくなった場合に場合に追加注文できるなどの利用が可能です。</p><p><img src="https://blog.gojiteji.com/images/jphacks2020/img1.png" alt="シャンプーなどを管理"></p><p>発表資料</p><iframe src="https://onedrive.live.com/embed?cid=B93604B8BA6D62A2&amp;resid=B93604B8BA6D62A2%21282644&amp;authkey=AAqrJwhDP-BBN1E&amp;em=2&amp;wdAr=1.7777777777777777" width="350px" height="221px" frameborder="0">これは、<a target="_blank" href="https://office.com/webapps">Office</a> の機能を利用した、<a target="_blank" href="https://office.com">Microsoft Office</a> の埋め込み型のプレゼンテーションです。</iframe><p>使用技術：<br>Vue.js Python heroku ESP32 など</p><h2 id="役職"><a href="#役職" class="headerlink" title="役職"></a>役職</h2><ul><li>買い物用の楽天API</li><li>arcana Largeの作成など</li></ul><h1 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h1><p>本選に出場できたため、finalistには選ばれたわけですが、残念ながらそれ以上の賞はありませんでした。(ぴえん)</p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>チームメンバーのおかげでCD&#x2F;CIやプロジェクト管理もしっかり行って開発でき、とても充実したイベントとなりました。また、フロントエンドはデザインからその実装まで行えるメンバーと開発でき、きれいなUIのアプリを作ることができました。。チームメンバーに感謝です！！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://blog.gojiteji.com/images/jphacks2020/title.png&quot;&gt;&lt;br&gt;お久しぶりです。&lt;br&gt;11&amp;#x2F;7にJPHACKS予選、その後11&amp;#x2F;28にJPHACKS本選に出場しました。その記録を書いておきます。(JPHACKS:&lt;a href=&quot;https://jphacks.com/&quot;&gt;https://jphacks.com/&lt;/a&gt;)&lt;/p&gt;</summary>
    
    
    
    
    <category term="hackathon" scheme="https://blog.gojtieji.com/tags/hackathon/"/>
    
  </entry>
  
  <entry>
    <title>Discovery hackathon 2020 参加記</title>
    <link href="https://blog.gojtieji.com/2020/10/26/discoveryhackathon/"/>
    <id>https://blog.gojtieji.com/2020/10/26/discoveryhackathon/</id>
    <published>2020-10-25T19:47:26.000Z</published>
    <updated>2022-04-22T22:49:23.182Z</updated>
    
    <content type="html"><![CDATA[<p>一月ほど経ってしまいましたが、9&#x2F;26,27に開催された<a href="https://www.discoverychannel.jp/campaign/hackathon2020/%60">Discovery Hackathon 2020</a>に参加しました。今年はコロナの影響もあり、オンライン開催となりました。テーマは <strong>「あたりまえ」をハックせよ</strong> でした。4人でチームを作って参加しました。(結果から言うと賞はありませんでした)<br><img src="https://blog.gojiteji.com/images/dhack/title.png" alt="200M"></p><span id="more"></span><h2 id="作ったもの"><a href="#作ったもの" class="headerlink" title="作ったもの"></a>作ったもの</h2><p>今回「200M」という、「相手の空間をのぞけるカメラ」を作りました。テーマっぽく表現すると「空間のあたりまえをハックした」ものです。</p><p><img src="https://blog.gojiteji.com/images/dhack/howitworks.png" alt="発表資料-動作概要"></p><p>1対１でビデオ通話する際、相手の顔は平面的に見え、実際に対面で話す際の３次元的な情報とは差異が発生します。そこで、「200M」では一方がカメラを様々な視点で覗くと、相手のカメラが移動することで、あたかも対面しているかのような視点を再現しました。</p><p>動作中の画像はこんなこんな感じ↓(GIF)<br><img src="https://blog.gojiteji.com/images/dhack/camera.gif" alt="通話先の相手の顔の位置に応じて動くカメラ"></p><p>システムとしては以下の画像のものです。私はラズパイでカメラを左右に移動する部分を作りました。フロント担当のメンバーに顔の位置認識を実装してもらい、その位置情報からkeiganモーターの回転数を決めて、カメラを紐で左右に移動させました。本来はカメラの角度と倍率を調節したかったのですが、モータとカメラの接着部が重みに耐えらず、何度も破損しては修正を繰り返すうちに終了時間が来てしまい、実装できませんでした。</p><p><img src="https://blog.gojiteji.com/images/dhack/system.png" alt="発表資料-構成案"></p><h2 id="所感など"><a href="#所感など" class="headerlink" title="所感など"></a>所感など</h2><p>結果としては、最初にも書いた通り賞はもらえませんでした。一方で審査員長が気に入ってくれた作品として終了後にコメントをくれたので、自分としては審査員長賞をもらったものと思い込んで納得させています。そろそろハッカソンで賞をとりたい。。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;一月ほど経ってしまいましたが、9&amp;#x2F;26,27に開催された&lt;a href=&quot;https://www.discoverychannel.jp/campaign/hackathon2020/%60&quot;&gt;Discovery Hackathon 2020&lt;/a&gt;に参加しました。今年はコロナの影響もあり、オンライン開催となりました。テーマは &lt;strong&gt;「あたりまえ」をハックせよ&lt;/strong&gt; でした。4人でチームを作って参加しました。(結果から言うと賞はありませんでした)&lt;br&gt;&lt;img src=&quot;https://blog.gojiteji.com/images/dhack/title.png&quot; alt=&quot;200M&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="hackathon" scheme="https://blog.gojtieji.com/tags/hackathon/"/>
    
  </entry>
  
  <entry>
    <title>Internship at Rakuten, Inc.</title>
    <link href="https://blog.gojtieji.com/2020/09/26/r-intern/"/>
    <id>https://blog.gojtieji.com/2020/09/26/r-intern/</id>
    <published>2020-09-25T16:30:15.000Z</published>
    <updated>2022-04-22T22:49:23.183Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://blog.gojiteji.com/images/r_intern/0.png"></p><p>While 9&#x2F;14-25, I participated Rakuten Internship which is called “Hutagotamagawa Natsu no Jin 2020”.( <a href="https://commerce-engineer.rakuten.careers/careers/graduates/internship/summer-short">https://commerce-engineer.rakuten.careers/careers/graduates/internship/summer-short</a> ). The Internship was hackathon style and taken play with 6 members and two mentors who were employees of the campany. Thankfully, they were very kind persons and it was very fun to be with them.</p><span id="more"></span><h2 id="Issue-🤔"><a href="#Issue-🤔" class="headerlink" title="Issue 🤔"></a>Issue 🤔</h2><p>A topic we received for the hackathon was “Connect with people online”. We discussed this topic and found a issue that it’s difficult to concentrate to do tasks such as work, schools and coding alone. Then, we decided to develop “virtual co-working space” , which visualize how much they worked within a certain amount of time and we named it “Priog.cafe”. “Prog” stands for “Programmer &amp; Progress”.<br><img src="https://blog.gojiteji.com/images/r_intern/1.png"></p><h2 id="Product-Design-amp-System-Design-📝"><a href="#Product-Design-amp-System-Design-📝" class="headerlink" title="Product Design &amp; System Design 📝"></a>Product Design &amp; System Design 📝</h2><p>Nexst to finding the problem, we discussed how we create the product which meats above conditions. We used miro and jira to organize a d realize our idea in this time. I’m in charge of the back-end work, so I illustrated sequence diagrams. We, back-end team chose to use redisDB, python http&#x2F;websocket server to make this.(Unfortunately, the separation of the http and websocket servers, which I will describe later, was fatal. I’m very sorry about that to all the members.)I’m not sure about  front-end, but i thik they used Vue &amp; TypeScript for coding. I’m thankful to have someone who can do design in addition to backend and frontend engineers in our team members.</p><p><img src="https://blog.gojiteji.com/images/r_intern/cont.png"></p><h2 id="development-👨‍💻"><a href="#development-👨‍💻" class="headerlink" title="development 👨‍💻"></a>development 👨‍💻</h2><p>In order to Use the above techniques, I made a search and signup functions by the http protocol. I made it possible to run them from the front end by accessing to the cloudfunction.I also wrote documentation on how to implement it for the backend team.<br>Next, I worked on a websocket communication server which is deploied via herokum. I’ve ever coded websocket in golang before, but I don’t have a lot of experience with asynchronous processing in python, so it was a bit tricky. It took me a long time to figure out why, especially since the variables that manage the workspace (room) were behaving differently for each websocket connection. The reason was that the codes didn’t have memory management for asynchronous processing. Creating a dictionary instance from the Manager in the multi processor library probably would have made this work, but it was causing a delay in the process, so one of the front-end member had to create one for me which used a nodejs server as a substitute. If I had been aware of the shared memory from the beginning, it wouldn’t have been a big problem.</p><p><img src="https://blog.gojiteji.com/images/r_intern/tech.png"></p><h2 id="Final-Product-amp-Presentation-🗣"><a href="#Final-Product-amp-Presentation-🗣" class="headerlink" title="Final Product &amp; Presentation 🗣"></a>Final Product &amp; Presentation 🗣</h2><p>The final product is shown in the image below. The working time is represented by the size of the fire in each account, and animations move during the work. It is possible to search, make personal calls and chat using webRTC. The login is through GitHub. The purpose of this is to limit the users to engineers.The judges gave it high marks and it won 1st place🎉🎉🎉.</p><p><img src="https://blog.gojiteji.com/images/r_intern/2.gif"></p><p><img src="https://blog.gojiteji.com/images/r_intern/3.png"></p><h2 id="Final-thoughts-💬"><a href="#Final-thoughts-💬" class="headerlink" title="Final thoughts 💬"></a>Final thoughts 💬</h2><p>Fully remote team development was a great experience.　I’m simply not good at team development, which is developing a given feature in a clearly defined deadline, but doing it online was extremely difficult to work with. I’d like to thank the team for their broad-mindedness. Especially I felt it when they allows me for extending the code freeze time a bit. (Of course I won’t do the impossibility of implementation in the design phase to avoid that in the future.)<br>As I said at the beginning, the mentors were very kind, and they were very knowledgeable about both the company and the technical world, and although it may be different in each department because it’s a big company, I could feel the good atmosphere of the team.<br>Once again, I am filled with gratitude to the people who were involved in this internship. The safety of asynchronous communication data, which I learned this time, is my subject to be studied in the future.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://blog.gojiteji.com/images/r_intern/0.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;While 9&amp;#x2F;14-25, I participated Rakuten Internship which is called “Hutagotamagawa Natsu no Jin 2020”.( &lt;a href=&quot;https://commerce-engineer.rakuten.careers/careers/graduates/internship/summer-short&quot;&gt;https://commerce-engineer.rakuten.careers/careers/graduates/internship/summer-short&lt;/a&gt; ). The Internship was hackathon style and taken play with 6 members and two mentors who were employees of the campany. Thankfully, they were very kind persons and it was very fun to be with them.&lt;/p&gt;</summary>
    
    
    
    
    <category term="internship" scheme="https://blog.gojtieji.com/tags/internship/"/>
    
    <category term="hackathon" scheme="https://blog.gojtieji.com/tags/hackathon/"/>
    
  </entry>
  
</feed>
